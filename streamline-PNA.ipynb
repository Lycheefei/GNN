{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8235654-b413-4fa7-b47a-9aa5fabee117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8135d4e4-f270-4c53-b82c-ea4bf2e7d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "qm9 = QM9(root='dataset/QM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b1a8ce-10f2-466f-b76b-5bdc2f95e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
    "\n",
    "qm9 = qm9.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782763e8-3183-44de-8b73-a20a14c94e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 30000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ca3067-1eb8-4638-a048-c97291e0f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45162416-e739-42c5-80fb-c14c79f65389",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd02308e-a726-4e87-a634-a3aa2d6afc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PNAConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg):\n",
    "        super(PNANet, self).__init__()\n",
    "        \n",
    "        # Define PNA layers with specified aggregators, scalers, and degree tensor\n",
    "        self.conv1 = PNAConv(\n",
    "            in_channels=num_node_features,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv2 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv3 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        \n",
    "        # Define linear layers for final graph-level output\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through PNA layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deab30a-48d2-4e44-aa60-9f2f18af9868",
   "metadata": {},
   "source": [
    "# Target 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81df5ac1-169f-4743-af5b-c21332531e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "target = 0\n",
    "qm9.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9.data.y[0:train_index].mean()\n",
    "data_std = qm9.data.y[0:train_index].std()\n",
    "\n",
    "qm9.data.y = (qm9.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2374c38a-b3e0-4450-8971-846ab88987c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9.num_features\n",
    "dim_h = 64\n",
    "edge_attr = qm9[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebcdaf78-911f-43ce-b889-5130be789a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
    "        model (nn.Module): GNN model to train on\n",
    "        loss (nn.functional): loss function to use during training\n",
    "        optimizer (torch.optim): optimizer during training\n",
    "\n",
    "    Returns:\n",
    "        float: training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        current_loss += l / len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69ea674b-2b2a-43b5-8b16-95023ca99124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): validation set in batches\n",
    "        model (nn.Module): current trained model\n",
    "        loss (nn.functional): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d952f615-ab2f-449a-ae67-82ffc9a32652",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): test dataset\n",
    "        model (nn.Module): trained model\n",
    "\n",
    "    Returns:\n",
    "        float: test loss\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # NOTE\n",
    "        # out = out.view(d.y.size())\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c6fc786-1cba-4204-a3cd-bf66d912dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    \"\"\"Training over all epochs\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs to train for\n",
    "        model (nn.Module): the current model\n",
    "        train_loader (DataLoader): training data in batches\n",
    "        val_loader (DataLoader): validation data in batches\n",
    "        path (string): path to save the best model\n",
    "\n",
    "    Returns:\n",
    "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.L1Loss()\n",
    "\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loader, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "        train_loss[epoch] = epoch_loss.detach().cpu().numpy()\n",
    "        val_loss[epoch] = v_loss.detach().cpu().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if epoch % 5 == 0:\n",
    "            print(\n",
    "                \"Epoch: \"\n",
    "                + str(epoch)\n",
    "                + \", Train loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return best_loss, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c63442e-ea76-4a8e-9db0-ef29678b1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.4855073392391205, Val loss: 0.45234110951423645\n",
      "Epoch: 10, Train loss: 0.48221397399902344, Val loss: 0.4539158344268799\n",
      "Epoch: 15, Train loss: 0.4730902910232544, Val loss: 0.4520643651485443\n",
      "Epoch: 20, Train loss: 0.46881183981895447, Val loss: 0.47190800309181213\n",
      "Epoch: 25, Train loss: 0.4659658670425415, Val loss: 0.4571883976459503\n",
      "Epoch: 30, Train loss: 0.46022576093673706, Val loss: 0.47000473737716675\n",
      "Epoch: 35, Train loss: 0.45941585302352905, Val loss: 0.431653767824173\n",
      "Epoch: 40, Train loss: 0.45958787202835083, Val loss: 0.4693063497543335\n",
      "Epoch: 45, Train loss: 0.45121821761131287, Val loss: 0.45378926396369934\n",
      "Epoch: 50, Train loss: 0.44882744550704956, Val loss: 0.4332180917263031\n",
      "Epoch: 55, Train loss: 0.44909578561782837, Val loss: 0.4616988003253937\n",
      "Epoch: 60, Train loss: 0.44400152564048767, Val loss: 0.43069541454315186\n",
      "Epoch: 65, Train loss: 0.44751715660095215, Val loss: 0.43895646929740906\n",
      "Epoch: 70, Train loss: 0.4426189064979553, Val loss: 0.4253494441509247\n",
      "Epoch: 75, Train loss: 0.43994849920272827, Val loss: 0.4326293468475342\n",
      "Epoch: 80, Train loss: 0.4399639070034027, Val loss: 0.4154151976108551\n",
      "Epoch: 85, Train loss: 0.43946582078933716, Val loss: 0.41748151183128357\n",
      "Epoch: 90, Train loss: 0.43585190176963806, Val loss: 0.4294430613517761\n",
      "Epoch: 95, Train loss: 0.4412705898284912, Val loss: 0.443746954202652\n",
      "Epoch: 100, Train loss: 0.4299302101135254, Val loss: 0.4314969778060913\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3da98af-7316-418d-9765-ac632d1d0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/3980023614.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for GIN: 0.34658658504486084\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90176fce-beb9-463d-932a-3d3729ed60bb",
   "metadata": {},
   "source": [
    "## Extra Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7024fb1-103b-4907-89e4-c0739012ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "def networkx_to_pyg_with_attributes(G):\n",
    "    # Convert NetworkX graph to PyG Data object with node and edge attributes\n",
    "    data = from_networkx(G, group_node_attrs=['x'], group_edge_attrs=['edge_attr'])\n",
    "\n",
    "    # If there are global attributes stored in G.graph, add them back to the PyG Data object\n",
    "    if 'y' in G.graph:\n",
    "        data.y = torch.tensor(G.graph['y']) if isinstance(G.graph['y'], list) else G.graph['y']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb78b55f-028c-48ed-99ab-e0c0c8314668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def pyg_to_networkx_with_attributes(data):\n",
    "    # Convert PyG data to NetworkX graph, retaining node attributes\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    # Add global attributes manually (if any exist)\n",
    "    G.graph['y'] = data.y\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f3f3b7a-95bb-4762-b319-f5c5a2ac62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def add_extra_node_on_each_edge(data):\n",
    "    # Convert PyG data to NetworkX graph for easier manipulation\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "    \n",
    "    # Original number of nodes\n",
    "    num_original_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Prepare lists for new features\n",
    "    edges = list(G.edges(data=True))\n",
    "    new_node_features = []\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_edge_features = []\n",
    "    original_edge_attrs = list(data.edge_attr)  # Store original edge attributes\n",
    "\n",
    "    # Keep track of removed edges to exclude their attributes\n",
    "    removed_edge_indices = []\n",
    "\n",
    "    for idx, (u, v, edge_data) in enumerate(edges):\n",
    "        # Remove the original edge and track its index for exclusion\n",
    "        G.remove_edge(u, v)\n",
    "        removed_edge_indices.append(idx)\n",
    "\n",
    "        # Create new node as the mean of connected node features\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        new_node_feature = (data.x[u] + data.x[v]) / 2\n",
    "        new_node_features.append(new_node_feature)\n",
    "        \n",
    "        # Add new node with feature\n",
    "        G.add_node(new_node_id, x=new_node_feature)\n",
    "\n",
    "        # Add edges from new node to each original node\n",
    "        G.add_edge(u, new_node_id)\n",
    "        G.add_edge(new_node_id, v)\n",
    "\n",
    "        # Use original edge feature for each new edge\n",
    "        edge_feature = edge_data['edge_attr']\n",
    "        edge_feature_tensor = (\n",
    "            edge_feature if isinstance(edge_feature, torch.Tensor) else torch.tensor(edge_feature)\n",
    "        )\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (u, new_node_id)\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (new_node_id, v)\n",
    "    \n",
    "    # Filter out removed edges from the original edge_attr\n",
    "    kept_edge_attr = [attr for i, attr in enumerate(original_edge_attrs) if i not in removed_edge_indices]\n",
    "\n",
    "    # Convert back to PyG Data object\n",
    "    modified_data = from_networkx(G)\n",
    "\n",
    "    # Update node features\n",
    "    modified_data.x = torch.cat([data.x, torch.stack(new_node_features)], dim=0)\n",
    "\n",
    "    # Update edge features: combine kept edge features with new edge features\n",
    "    modified_data.edge_attr = torch.cat([torch.stack(kept_edge_attr), torch.stack(new_edge_features)], dim=0)\n",
    "\n",
    "    modified_data.y = data.y\n",
    "    \n",
    "    return modified_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef8ec4d4-de01-445c-97d0-43c7af2e0440",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ExN_set \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(qm9[:\u001b[38;5;241m30000\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m ExN_set:  \u001b[38;5;66;03m# Process 100 molecules as an example\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     modified_data \u001b[38;5;241m=\u001b[39m \u001b[43madd_extra_node_on_each_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     qm9_ExN\u001b[38;5;241m.\u001b[39mappend(modified_data)\n",
      "Cell \u001b[0;32mIn[62], line 58\u001b[0m, in \u001b[0;36madd_extra_node_on_each_edge\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     55\u001b[0m modified_data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data\u001b[38;5;241m.\u001b[39mx, torch\u001b[38;5;241m.\u001b[39mstack(new_node_features)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Update edge features: combine kept edge features with new edge features\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m modified_data\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkept_edge_attr\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mstack(new_edge_features)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m modified_data\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modified_data\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Apply the transformation across a subset of QM9\n",
    "qm9_ExN = []\n",
    "ExN_set = copy.deepcopy(qm9[:30000])\n",
    "for data in ExN_set:  # Process 100 molecules as an example\n",
    "    modified_data = add_extra_node_on_each_edge(data)\n",
    "    qm9_ExN.append(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81f4ea2f-0f91-420c-9359-4f7ba9baa42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 11], edge_index=[2, 36], edge_attr=[36, 4], y=[1], pos=[19, 3], z=[19], smiles='[H]C(=O)OC([H])([H])C([H])([H])[C@@]([H])(C([H])=O)C([H])([H])[H]', name='gdb_62256', idx=[1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExN_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f038d6f-1497-4c83-a3f2-8b99df360134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[55, 11], edge_index=[2, 72], y=[1], edge_attr=[108, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_ExN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318c710-6f8c-4af9-8c60-3a3965cd547b",
   "metadata": {},
   "source": [
    "## Subgraph Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "975fa655-b6f7-40b9-af9c-dd0ea84cc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_subgraph_features_with_edges(data, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Initialize lists to store node and edge features\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "    edge_subgraph_centralities = []\n",
    "\n",
    "    # Compute node-level subgraph features\n",
    "    for node in G.nodes():\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "\n",
    "    # Convert node features to tensors\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "    data.x = torch.cat([data.x, subgraph_sizes_tensor, subgraph_degrees_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge-level subgraph features\n",
    "    for u, v in G.edges():\n",
    "        subgraph_u = nx.ego_graph(G, u, radius=radius)\n",
    "        edge_centrality = nx.edge_betweenness_centrality(subgraph_u, normalized=True).get((u, v), 0)\n",
    "        edge_subgraph_centralities.append(edge_centrality)\n",
    "\n",
    "    # Convert edge features to tensor and concatenate to edge_attr\n",
    "    edge_centrality_tensor = torch.tensor(edge_subgraph_centralities, dtype=torch.float).view(-1, 1)\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7124e2f-e4c0-4c9d-be27-90666207dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_data = qm9[:30000]\n",
    "qm9_sub = copy.deepcopy(qm9_data)\n",
    "qm9_SE = []\n",
    "for data in qm9_sub:\n",
    "    data_sub = extract_local_subgraph_features_with_edges(data, radius=2)\n",
    "    qm9_SE.append(data_sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cab34313-a0b4-44e3-80f3-8166c07796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_SE[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_SE[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_SE[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71674cdf-b330-4440-887b-8290b928e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_SE[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecd748c2-811b-4520-89e4-67bcb97659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_SE[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_SE[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_SE[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9843a91e-ff69-47ca-bf12-8294622ed687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.5388436317443848, Val loss: 0.47594335675239563\n",
      "Epoch: 10, Train loss: 0.502517580986023, Val loss: 0.481692373752594\n",
      "Epoch: 15, Train loss: 0.4866790473461151, Val loss: 0.4826018214225769\n",
      "Epoch: 20, Train loss: 0.4694737493991852, Val loss: 0.4423438608646393\n",
      "Epoch: 25, Train loss: 0.467204213142395, Val loss: 0.44274091720581055\n",
      "Epoch: 30, Train loss: 0.45695260167121887, Val loss: 0.435397207736969\n",
      "Epoch: 35, Train loss: 0.45430871844291687, Val loss: 0.43345534801483154\n",
      "Epoch: 40, Train loss: 0.448910117149353, Val loss: 0.44769564270973206\n",
      "Epoch: 45, Train loss: 0.4464457631111145, Val loss: 0.4617311358451843\n",
      "Epoch: 50, Train loss: 0.44150856137275696, Val loss: 0.44321370124816895\n",
      "Epoch: 55, Train loss: 0.4415639638900757, Val loss: 0.4282848536968231\n",
      "Epoch: 60, Train loss: 0.43270841240882874, Val loss: 0.41854554414749146\n",
      "Epoch: 65, Train loss: 0.4346553385257721, Val loss: 0.43637746572494507\n",
      "Epoch: 70, Train loss: 0.43330496549606323, Val loss: 0.4213460385799408\n",
      "Epoch: 75, Train loss: 0.4297485649585724, Val loss: 0.43056541681289673\n",
      "Epoch: 80, Train loss: 0.4282503128051758, Val loss: 0.4272023141384125\n",
      "Epoch: 85, Train loss: 0.42712658643722534, Val loss: 0.4269062876701355\n",
      "Epoch: 90, Train loss: 0.42428407073020935, Val loss: 0.4143683910369873\n",
      "Epoch: 95, Train loss: 0.4219164550304413, Val loss: 0.40571901202201843\n",
      "Epoch: 100, Train loss: 0.4237443506717682, Val loss: 0.4211895763874054\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_SE.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be3fc739-3ac9-4993-a28f-ce220283985c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load our model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPNANet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPNA_0_model_SE.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# calculate test loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mPNANet.__init__\u001b[0;34m(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28msuper\u001b[39m(PNANet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define PNA layers with specified aggregators, scalers, and degree tensor\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m \u001b[43mPNAConv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_node_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscalers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m PNAConv(\n\u001b[1;32m     20\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39mdim_h,\n\u001b[1;32m     21\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mdim_h,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     edge_dim\u001b[38;5;241m=\u001b[39medge_attr\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m PNAConv(\n\u001b[1;32m     28\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39mdim_h,\n\u001b[1;32m     29\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mdim_h,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     edge_dim\u001b[38;5;241m=\u001b[39medge_attr\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/pna_conv.py:109\u001b[0m, in \u001b[0;36mPNAConv.__init__\u001b[0;34m(self, in_channels, out_channels, aggregators, scalers, deg, edge_dim, towers, pre_layers, post_layers, divide_input, act, act_kwargs, train_norm, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     in_channels: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    107\u001b[0m ):\n\u001b[0;32m--> 109\u001b[0m     aggr \u001b[38;5;241m=\u001b[39m \u001b[43mDegreeScalerAggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(aggr\u001b[38;5;241m=\u001b[39maggr, node_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m divide_input:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/aggr/scaler.py:59\u001b[0m, in \u001b[0;36mDegreeScalerAggregation.__init__\u001b[0;34m(self, aggr, scaler, deg, train_norm, aggr_kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(deg\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     57\u001b[0m bin_degree \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(deg\u001b[38;5;241m.\u001b[39mnumel(), device\u001b[38;5;241m=\u001b[39mdeg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_avg_deg_lin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_degree\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_avg_deg_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(((bin_degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;241m*\u001b[39m deg)\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m/\u001b[39m N\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_norm:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_SE.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb73999-6fce-4804-b686-613b9978881d",
   "metadata": {},
   "source": [
    "## Virtual Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9d3716e-c158-45ec-8552-41cd8f68e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import VirtualNode\n",
    "import copy\n",
    "\n",
    "qm9_vn = copy.deepcopy(qm9)\n",
    "\n",
    "transform = VirtualNode()\n",
    "qm9_vn.transform = transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a62264fc-b715-4869-b9d3-c10908395d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_vn[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_vn[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_vn[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "840e961b-0849-41c0-b493-d25046b9cdf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m qm9_vn[:train_index]:\n\u001b[1;32m      9\u001b[0m     d \u001b[38;5;241m=\u001b[39m degree(data\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m], num_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mnum_nodes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 10\u001b[0m     deg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbincount(d, minlength\u001b[38;5;241m=\u001b[39mdeg\u001b[38;5;241m.\u001b[39mnumel())\n\u001b[1;32m     12\u001b[0m aggregators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m scalers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamplification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattenuation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_vn[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_vn[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_vn[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbc434-d182-43a9-8b86-d0fa846e6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_vn.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efe2b1-e9bf-4d8b-9b4a-e73d9e1eda29",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b0dc9-f0cf-4ef3-bc62-bb318ceb1786",
   "metadata": {},
   "source": [
    "### Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d1dd0-31ae-4585-9d50-0d6f36aaeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def pyg_to_networkx_with_attributes(data):\n",
    "    # Convert PyG data to NetworkX graph, retaining node attributes\n",
    "    G = to_networkx(data, node_attrs=['x', 'pos', 'z'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    # Add global attributes manually (if any exist)\n",
    "    G.graph['y'] = data.y\n",
    "    G.graph['smiles'] = data.smiles\n",
    "    G.graph['name'] = data.name\n",
    "    G.graph['idx'] = data.idx\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323567b-e1fc-4d62-a7d1-3d8e3571a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_betweenness_centrality(data):\n",
    "    # Convert PyG data to NetworkX graph, preserving attributes\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute node betweenness centrality\n",
    "    node_betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "    node_centrality_values = list(node_betweenness.values())\n",
    "    node_centrality_tensor = torch.tensor(node_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append node betweenness centrality to node features\n",
    "    data.x = torch.cat([data.x, node_centrality_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "    edge_centrality_values = [edge_betweenness[(u, v)] for u, v in G.edges()]\n",
    "    edge_centrality_tensor = torch.tensor(edge_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append edge betweenness centrality to edge features\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141bc8a-464d-4ff8-964d-1fb3e24beab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_data = qm9[:data_size]\n",
    "qm9_centrality = copy.deepcopy(qm9_data)\n",
    "qm9_bet = []\n",
    "for data in qm9_centrality:\n",
    "    data_bet = add_betweenness_centrality(data)\n",
    "    qm9_bet.append(data_bet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5287828-a73a-4062-a875-89f303d520f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_bet[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_bet[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_bet[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99d53b-5cb9-48fc-80d9-def1500b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_bet[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_bet[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_bet[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e19c8-db4a-45cd-b348-b2169c4c2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_bet.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fbc6c-8816-4cab-a96d-d5227a7872d0",
   "metadata": {},
   "source": [
    "### Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21e406-e273-4c41-b7ae-218aa0e5e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centrality_to_node_features(data, centrality_measure='degree'):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "\n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = list(centrality.values())\n",
    "    centrality_tensor = (centrality_tensor - centrality_tensor.mean()) / (centrality_tensor.std() + 1e-8)\n",
    "    data.x = torch.cat([data.x, centrality_tensor], dim=-1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbb992-a24f-45e9-b202-09f3f4588874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_deg = []\n",
    "for data in qm9_centrality:\n",
    "    data_deg = add_centrality_to_node_features(data, centrality_measure='degree')\n",
    "    qm9_deg.append(data_deg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c81638-70f0-4cfc-88e6-79ce72db82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_deg[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_deg[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_deg[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cc60b-6cc1-4ccc-a9ff-c0aed933a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_deg[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_deg[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_deg[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfa02b-b6f7-486e-9dde-6c153113eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_deg.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce87e5-a7d9-403e-be5c-87dd81e90a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_clo = []\n",
    "for data in qm9_centrality:\n",
    "    data_clo = add_centrality_to_node_features(data, centrality_measure='closeness')\n",
    "    qm9_clo.append(data_deg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dad76c-a860-4b50-bd0b-7bb8bf5ce6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_clo[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_clo[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_clo[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbe8a6-279c-4d73-b32b-26a5c95d6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_clo[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_clo[0].edge_attr.shape[1]\n",
    "from torch_geometric.utils import degree\n",
    "# Compute in-degree histogram over training data.\n",
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in qm9_clo[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39730a43-92ff-4966-91b8-9c8da63a95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_clo.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae22bf-83aa-495d-839f-80874c83395e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2fbfd-c25f-4e1f-9084-c1e17a3403d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046aae0f-a590-4f01-a643-efe5fe1fc491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f27c91-3812-49ae-acac-f39f37c7c3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b4b94f-809b-4317-a221-54cf8b70827e",
   "metadata": {},
   "source": [
    "# Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0193a8f8-a9ac-474d-82a0-333511f81157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c3a83c6-8e7d-4d02-83e6-ff497ef00b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9 = QM9(root='dataset/QM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b74b6a09-51c1-46b0-abd2-e213f5893f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
    "\n",
    "qm9 = qm9.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93a373b8-7b54-428e-a7e6-c12be8927ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 30000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12e19c2c-47fe-4e8f-adc2-a1202c15d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_dataset = qm9[:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbbe6149-39d5-49ba-a1bd-35813e225a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 11], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447ca51-811f-4152-bc3c-0bccc71c8ab7",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e52adf-74e8-4875-af7b-319982b6d272",
   "metadata": {},
   "source": [
    "## Virtual Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "470661e0-88cb-434e-a28c-ea29c36c6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import VirtualNode\n",
    "import copy\n",
    "\n",
    "qm9_vn = copy.deepcopy(qm9_dataset)\n",
    "\n",
    "transform = VirtualNode()\n",
    "qm9_vn.transform = transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5030f3a-3861-4d16-b1a1-fc283148dc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[22, 11], edge_index=[2, 86], edge_attr=[86, 4], y=[1, 19], pos=[22, 3], z=[22], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1], edge_type=[86])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_vn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ac6de-29f4-4777-8de1-9f6d02cd39f0",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c89cc-bd49-4d4d-a0fa-1e4a3789ad0e",
   "metadata": {},
   "source": [
    "### Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "248bf2cc-85bb-43cc-933c-49eb68f63093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def pyg_to_networkx_with_attributes(data):\n",
    "    # Convert PyG data to NetworkX graph, retaining node attributes\n",
    "    G = to_networkx(data, node_attrs=['x', 'pos', 'z'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    # Add global attributes manually (if any exist)\n",
    "    G.graph['y'] = data.y\n",
    "    G.graph['smiles'] = data.smiles\n",
    "    G.graph['name'] = data.name\n",
    "    G.graph['idx'] = data.idx\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45270ffd-5e62-446a-8ff9-bd25f5f8ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_betweenness_centrality(data):\n",
    "    # Convert PyG data to NetworkX graph, preserving attributes\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute node betweenness centrality\n",
    "    node_betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "    node_centrality_values = list(node_betweenness.values())\n",
    "    node_centrality_tensor = torch.tensor(node_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append node betweenness centrality to node features\n",
    "    data.x = torch.cat([data.x, node_centrality_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "    edge_centrality_values = [edge_betweenness[(u, v)] for u, v in G.edges()]\n",
    "    edge_centrality_tensor = torch.tensor(edge_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append edge betweenness centrality to edge features\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd1d33e9-c1ff-4140-b1cd-42cb5d4a0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_centrality = copy.deepcopy(qm9_dataset)\n",
    "qm9_bet = []\n",
    "for data in qm9_centrality:\n",
    "    data_bet = add_betweenness_centrality(data)\n",
    "    qm9_bet.append(data_bet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59b8676f-e796-47b4-bae8-e5612da7b849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 12], edge_index=[2, 44], edge_attr=[44, 5], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_bet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c093db-d0b2-42bd-8e41-26e8f4e12c5a",
   "metadata": {},
   "source": [
    "### Degree, Closeness, Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "56bd068d-91dc-4527-87a2-54e3beac5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centrality_to_node_features(data, centrality_measure='degree'):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if G.is_directed():\n",
    "            G = G.to_undirected()\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "\n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = list(centrality.values())\n",
    "    centrality_tensor = torch.tensor(centrality_values, dtype=torch.float).view(-1, 1)\n",
    "    centrality_tensor = (centrality_tensor - centrality_tensor.mean()) / (centrality_tensor.std() + 1e-8)\n",
    "    data.x = torch.cat([data.x, centrality_tensor], dim=-1)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b89c714b-568a-4131-8be3-fc3697b4e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_deg = []\n",
    "for data in qm9_centrality:\n",
    "    data_deg = add_centrality_to_node_features(data, centrality_measure='degree')\n",
    "    qm9_deg.append(data_deg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "022de59d-cb27-4070-b968-eae8d887c566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 12], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_deg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "38a9bc8f-653c-445f-bb58-665da6d5da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_clo = []\n",
    "for data in qm9_centrality:\n",
    "    data_clo = add_centrality_to_node_features(data, centrality_measure='closeness')\n",
    "    qm9_clo.append(data_clo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b0e1c5d-ed88-49c2-9639-69ed20abffd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 12], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_clo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e29a2b55-2bba-42f0-a7d8-efd47ade01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_eig = []\n",
    "for data in qm9_centrality:\n",
    "    data_eig = add_centrality_to_node_features(data, centrality_measure='eigenvector')\n",
    "    qm9_eig.append(data_eig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d28daf50-4035-43d5-a88e-6ec93f651880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 12], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_eig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684570ce-7cc1-4682-928d-103b575f00b2",
   "metadata": {},
   "source": [
    "## Subgraph Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1fd0f23-0365-44fb-a950-3842d963efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_subgraph_features_with_edges(data, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Initialize lists to store node and edge features\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "    edge_subgraph_centralities = []\n",
    "\n",
    "    # Compute node-level subgraph features\n",
    "    for node in G.nodes():\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "\n",
    "    # Convert node features to tensors\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "    data.x = torch.cat([data.x, subgraph_sizes_tensor, subgraph_degrees_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge-level subgraph features\n",
    "    for u, v in G.edges():\n",
    "        subgraph_u = nx.ego_graph(G, u, radius=radius)\n",
    "        edge_centrality = nx.edge_betweenness_centrality(subgraph_u, normalized=True).get((u, v), 0)\n",
    "        edge_subgraph_centralities.append(edge_centrality)\n",
    "\n",
    "    # Convert edge features to tensor and concatenate to edge_attr\n",
    "    edge_centrality_tensor = torch.tensor(edge_subgraph_centralities, dtype=torch.float).view(-1, 1)\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "edef7a4e-6062-49ad-83eb-014cebb9beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_sub = copy.deepcopy(qm9_dataset)\n",
    "qm9_SE = []\n",
    "for data in qm9_sub:\n",
    "    data_sub = extract_local_subgraph_features_with_edges(data, radius=2)\n",
    "    qm9_SE.append(data_sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "74fbee80-512f-4c36-9f02-adf6bde04105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 13], edge_index=[2, 44], edge_attr=[44, 5], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_SE[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b484d-74e1-4b88-a4b7-89bbe00b5fb9",
   "metadata": {},
   "source": [
    "## Graph Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e25d009-ee37-4229-a09f-644d437044ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "qm9_GE = copy.deepcopy(qm9_dataset)\n",
    "\n",
    "transform = AddLaplacianEigenvectorPE(k=2, attr_name = None)\n",
    "qm9_GE.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2919cca3-9659-4965-910f-58f74ef96c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 13], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_GE[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c426c1b-38e4-4af7-805f-f52bafeff2d7",
   "metadata": {},
   "source": [
    "## Extra Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3dcd21b9-3e86-45b6-90ce-59e0b2c19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def add_extra_node_on_each_edge(data):\n",
    "    # Convert PyG data to a NetworkX graph for easier manipulation\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs = ['edge_attr'])\n",
    "    \n",
    "    # Original number of nodes\n",
    "    num_original_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Prepare lists for new features\n",
    "    edges = list(G.edges(data=True))\n",
    "    new_node_features = []\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_edge_features = []\n",
    "\n",
    "    for u, v, edge_data in edges:\n",
    "        # Remove the original edge\n",
    "        G.remove_edge(u, v)\n",
    "\n",
    "        # Create new node as the mean of connected node features\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        new_node_feature = (data.x[u] + data.x[v]) / 2\n",
    "        new_node_features.append(new_node_feature)\n",
    "        \n",
    "        # Add new node with feature\n",
    "        G.add_node(new_node_id, x=new_node_feature)\n",
    "\n",
    "        # Add edges from new node to each original node\n",
    "        G.add_edge(u, new_node_id)\n",
    "        G.add_edge(new_node_id, v)\n",
    "\n",
    "        # Use original edge feature for each new edge\n",
    "        edge_feature = edge_data['edge_attr']\n",
    "        edge_feature_tensor = (\n",
    "            edge_feature if isinstance(edge_feature, torch.Tensor) else torch.tensor(edge_feature)\n",
    "        )\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (u, new_node_id)\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (new_node_id, v)\n",
    "    \n",
    "    # Convert back to PyG Data object\n",
    "    modified_data = from_networkx(G)\n",
    "\n",
    "    # Update node features\n",
    "    modified_data.x = torch.cat([data.x, torch.stack(new_node_features)], dim=0)\n",
    "\n",
    "    # Update edge features to include only the new edges\n",
    "    modified_data.edge_attr = torch.stack(new_edge_features)  # Only include new edge features\n",
    "\n",
    "    # Preserve any additional global attributes\n",
    "    modified_data.y = data.y\n",
    "    modified_data.smiles = data.smiles\n",
    "    modified_data.name = data.name\n",
    "    modified_data.idx = data.idx\n",
    "    modified_data.pos = data.pos\n",
    "    modified_data.z = data.z\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47b52f37-1adf-41e7-b44f-91e66cb726da",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_ExN = []\n",
    "ExN_set = copy.deepcopy(qm9_dataset)\n",
    "for data in ExN_set:  # Process 100 molecules as an example\n",
    "    modified_data = add_extra_node_on_each_edge(data)\n",
    "    qm9_ExN.append(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ce9f8476-9de0-4efa-b2b0-1c238c3b763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[65, 11], edge_index=[2, 88], edge_attr=[88, 4], y=[1, 19], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1], pos=[21, 3], z=[21])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_ExN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939fb47-4fe9-4389-bfcc-b24e9d5e24f5",
   "metadata": {},
   "source": [
    "## Distance Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d822b61-95f1-438e-af66-7da6c78f7dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "751064f2-c916-445f-a093-10e49e181dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795ee0f-2c7c-46d9-9db9-bbfd129156c8",
   "metadata": {},
   "source": [
    "## PNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "56e95772-0e07-41cc-9c32-f7f865ea41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PNAConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg):\n",
    "        super(PNANet, self).__init__()\n",
    "        \n",
    "        # Define PNA layers with specified aggregators, scalers, and degree tensor\n",
    "        self.conv1 = PNAConv(\n",
    "            in_channels=num_node_features,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv2 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv3 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        \n",
    "        # Define linear layers for final graph-level output\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through PNA layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d11fed-4db9-4a6f-8f37-d6a1b4385fa5",
   "metadata": {},
   "source": [
    "## GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a33282cb-1dfd-4bd0-adc1-d0fb3eb8f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class GINENet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr):\n",
    "        super(GINENet, self).__init__()\n",
    "        \n",
    "        # Define GINE layers with the specified edge_dim\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        \n",
    "        # Define linear layers for classification or regression\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through GINE layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796473e-018f-4305-9e78-ecece16883db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87263c6a-eaa7-46f3-9849-c14afde70a96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a9ee6a77-d8ee-4a14-a699-ff07ca4b23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
    "        model (nn.Module): GNN model to train on\n",
    "        loss (nn.functional): loss function to use during training\n",
    "        optimizer (torch.optim): optimizer during training\n",
    "\n",
    "    Returns:\n",
    "        float: training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        current_loss += l / len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model\n",
    "\n",
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): validation set in batches\n",
    "        model (nn.Module): current trained model\n",
    "        loss (nn.functional): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "    return val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): test dataset\n",
    "        model (nn.Module): trained model\n",
    "\n",
    "    Returns:\n",
    "        float: test loss\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # NOTE\n",
    "        # out = out.view(d.y.size())\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dca71916-a38c-4896-b8ff-6b153df44f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    \"\"\"Training over all epochs\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs to train for\n",
    "        model (nn.Module): the current model\n",
    "        train_loader (DataLoader): training data in batches\n",
    "        val_loader (DataLoader): validation data in batches\n",
    "        path (string): path to save the best model\n",
    "\n",
    "    Returns:\n",
    "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.L1Loss()\n",
    "\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loader, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "        train_loss[epoch] = epoch_loss.detach().cpu().numpy()\n",
    "        val_loss[epoch] = v_loss.detach().cpu().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if epoch % 5 == 0:\n",
    "            print(\n",
    "                \"Epoch: \"\n",
    "                + str(epoch)\n",
    "                + \", Train loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return best_loss, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1175db5-2804-4e39-84aa-b255ee4a4a30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Target 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4b3202c-b949-4507-b221-8a3104b19bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "target = 0\n",
    "qm9_dataset.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_dataset.data.y = (qm9_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_dataset[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca8cadf5-d0c8-4706-a9e2-fe571b6ddf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 11], edge_index=[2, 44], edge_attr=[44, 4], y=[1], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d618e1e-a187-4883-a64d-c6e6af11d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "af120a10-96d4-42be-9b2b-1e051756a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ec3b8eb-7eff-417f-bf62-fc336c933575",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "00a60de5-fdd4-4043-b2ec-fd4dbca00982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.5401504635810852, Val loss: 0.5000472068786621\n",
      "Epoch: 10, Train loss: 0.5032700300216675, Val loss: 0.48793160915374756\n",
      "Epoch: 15, Train loss: 0.49474871158599854, Val loss: 0.46651947498321533\n",
      "Epoch: 20, Train loss: 0.4784812927246094, Val loss: 0.448507159948349\n",
      "Epoch: 25, Train loss: 0.4720373749732971, Val loss: 0.44866734743118286\n",
      "Epoch: 30, Train loss: 0.4631823003292084, Val loss: 0.44419562816619873\n",
      "Epoch: 35, Train loss: 0.4609650671482086, Val loss: 0.448944628238678\n",
      "Epoch: 40, Train loss: 0.4551342725753784, Val loss: 0.45710673928260803\n",
      "Epoch: 45, Train loss: 0.4479655623435974, Val loss: 0.4450874626636505\n",
      "Epoch: 50, Train loss: 0.4539875090122223, Val loss: 0.4534499943256378\n",
      "Epoch: 55, Train loss: 0.44478073716163635, Val loss: 0.4447292983531952\n",
      "Epoch: 60, Train loss: 0.44454485177993774, Val loss: 0.4304903745651245\n",
      "Epoch: 65, Train loss: 0.44563212990760803, Val loss: 0.43736425042152405\n",
      "Epoch: 70, Train loss: 0.4467959702014923, Val loss: 0.43986794352531433\n",
      "Epoch: 75, Train loss: 0.4444735050201416, Val loss: 0.42608553171157837\n",
      "Epoch: 80, Train loss: 0.4396636188030243, Val loss: 0.4358435869216919\n",
      "Epoch: 85, Train loss: 0.4373025894165039, Val loss: 0.4506157636642456\n",
      "Epoch: 90, Train loss: 0.4333745539188385, Val loss: 0.44061166048049927\n",
      "Epoch: 95, Train loss: 0.43683621287345886, Val loss: 0.4682995676994324\n",
      "Epoch: 100, Train loss: 0.4336262047290802, Val loss: 0.42642512917518616\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "708578b4-f659-4740-84ec-bfa9dbbefc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/1113277864.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.3822985887527466\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4552cf19-e367-4276-9e6b-502ab1167a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0\n",
    "qm9_vn.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_vn.data.y[0:train_index].mean()\n",
    "data_std = qm9_vn.data.y[0:train_index].std()\n",
    "\n",
    "qm9_vn.data.y = (qm9_vn.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_vn[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_vn[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_vn[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fef33868-6686-4e29-b626-73873d44a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_vn[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_vn[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "85ea13d0-4b47-4c92-b0cc-7d461f2f4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_vn[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_vn[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3c4a3baa-d980-4c4e-8131-329dc85035af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.5329497456550598, Val loss: 0.5096242427825928\n",
      "Epoch: 10, Train loss: 0.5075181722640991, Val loss: 0.5027762651443481\n",
      "Epoch: 15, Train loss: 0.49214962124824524, Val loss: 0.46368408203125\n",
      "Epoch: 20, Train loss: 0.47868314385414124, Val loss: 0.4604070782661438\n",
      "Epoch: 25, Train loss: 0.4696469008922577, Val loss: 0.46889686584472656\n",
      "Epoch: 30, Train loss: 0.46168792247772217, Val loss: 0.4555462598800659\n",
      "Epoch: 35, Train loss: 0.4575309455394745, Val loss: 0.46633151173591614\n",
      "Epoch: 40, Train loss: 0.4554443955421448, Val loss: 0.4480733275413513\n",
      "Epoch: 45, Train loss: 0.4494090676307678, Val loss: 0.4439085125923157\n",
      "Epoch: 50, Train loss: 0.44441458582878113, Val loss: 0.43937817215919495\n",
      "Epoch: 55, Train loss: 0.44128888845443726, Val loss: 0.430616557598114\n",
      "Epoch: 60, Train loss: 0.44125163555145264, Val loss: 0.42584362626075745\n",
      "Epoch: 65, Train loss: 0.43428319692611694, Val loss: 0.42056283354759216\n",
      "Epoch: 70, Train loss: 0.43393269181251526, Val loss: 0.46292006969451904\n",
      "Epoch: 75, Train loss: 0.43212324380874634, Val loss: 0.43760326504707336\n",
      "Epoch: 80, Train loss: 0.43020936846733093, Val loss: 0.4256380796432495\n",
      "Epoch: 85, Train loss: 0.4350750744342804, Val loss: 0.44552522897720337\n",
      "Epoch: 90, Train loss: 0.4324984550476074, Val loss: 0.4156249165534973\n",
      "Epoch: 95, Train loss: 0.4254840910434723, Val loss: 0.42555108666419983\n",
      "Epoch: 100, Train loss: 0.427460640668869, Val loss: 0.4723524749279022\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_vn.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7080d685-bf1b-4a19-9f48-0c95b860ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/3144575080.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_vn.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.34801554679870605\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_vn.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bac58912-86e5-4f7c-b639-dfa39e3712d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "# Define a custom InMemoryDataset to hold the processed QM9 data with betweenness centrality\n",
    "class CustomQM9Dataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomQM9Dataset, self).__init__()\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "# Create an InMemoryDataset from the list of transformed graphs\n",
    "qm9_bet_dataset = CustomQM9Dataset(qm9_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "174b1959-4b30-4c57-b05f-10955dcce2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 12], edge_index=[2, 44], edge_attr=[44, 5], y=[1, 19], pos=[21, 3], z=[21], smiles='[H]C([H])([H])C([H])([H])[C@]1([H])OC([H])([H])[C@]2([H])O[C@]2([H])C1([H])[H]', name='gdb_115914', idx=[1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_bet_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "85ba1a7d-18fe-4ad3-8d29-8b26e9a15063",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0\n",
    "qm9_bet_dataset.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_bet_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_bet_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_bet_dataset.data.y = (qm9_bet_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_bet_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_bet_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_bet_dataset[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f634c1a0-26f5-44d0-9621-2b8200515df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_bet_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_bet_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1851659a-fb66-435b-b98c-b7ed62f2817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_bet_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_bet_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9ac5af1b-42ce-45cf-8440-b42e918a8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.783633828163147, Val loss: 1.0525089502334595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pna_best_loss, pna_train_loss, pna_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNA_0_model_bet.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 27\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     25\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m v_loss\n\u001b[1;32m     26\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), path)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     28\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:204\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    201\u001b[0m         shape[cat_dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(slices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    202\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;241m*\u001b[39mshape)\n\u001b[0;32m--> 204\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, EdgeIndex) \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_sorted:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Check whether the whole `EdgeIndex` is sorted by row:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_sorted_by_row \u001b[38;5;129;01mand\u001b[39;00m (value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_bet.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "633364c5-205e-47bb-9252-f3969d4bcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_clo_dataset = CustomQM9Dataset(qm9_clo)\n",
    "\n",
    "target = 0\n",
    "qm9_clo_dataset.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_clo_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_clo_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_clo_dataset.data.y = (qm9_clo_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_clo_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_clo_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_clo_dataset[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2d71b409-5da1-47e8-9c6a-e1d902700bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_clo_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_clo_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "72b4bb0a-7a94-4b4c-bad4-3e8a6d0616ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_clo_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_clo_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4341d6c5-790b-4589-84fb-84935f546a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.7838258743286133, Val loss: 1.0532243251800537\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pna_best_loss, pna_train_loss, pna_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNA_0_model_clo.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 22\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs):\n\u001b[0;32m---> 22\u001b[0m     epoch_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     v_loss \u001b[38;5;241m=\u001b[39m validation(val_loader, model, loss)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[0;32mIn[117], line 25\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(loader, model, loss, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     l \u001b[38;5;241m=\u001b[39m loss(out, torch\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39my, (\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39my), \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     24\u001b[0m     current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_loss, model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"PNA_0_model_clo.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecb426-e28e-4fe2-b631-a7b7ff1e3647",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "deb3af33-c9dd-4880-b480-064a7048233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.500133752822876, Val loss: 0.508162260055542\n",
      "Epoch: 10, Train loss: 0.473743200302124, Val loss: 0.4478645622730255\n",
      "Epoch: 15, Train loss: 0.45926687121391296, Val loss: 0.4447709321975708\n",
      "Epoch: 20, Train loss: 0.44492265582084656, Val loss: 0.428652286529541\n",
      "Epoch: 25, Train loss: 0.43891581892967224, Val loss: 0.43907877802848816\n",
      "Epoch: 30, Train loss: 0.43219250440597534, Val loss: 0.40929269790649414\n",
      "Epoch: 35, Train loss: 0.42880168557167053, Val loss: 0.42619621753692627\n",
      "Epoch: 40, Train loss: 0.42239511013031006, Val loss: 0.41917648911476135\n",
      "Epoch: 45, Train loss: 0.4182688593864441, Val loss: 0.4052686095237732\n",
      "Epoch: 50, Train loss: 0.41259950399398804, Val loss: 0.39702659845352173\n",
      "Epoch: 55, Train loss: 0.4112132489681244, Val loss: 0.3918880522251129\n",
      "Epoch: 60, Train loss: 0.40784984827041626, Val loss: 0.4035443365573883\n",
      "Epoch: 65, Train loss: 0.4021839201450348, Val loss: 0.41469377279281616\n",
      "Epoch: 70, Train loss: 0.4013875722885132, Val loss: 0.3935644328594208\n",
      "Epoch: 75, Train loss: 0.3995414078235626, Val loss: 0.38624900579452515\n",
      "Epoch: 80, Train loss: 0.3953920304775238, Val loss: 0.3853675127029419\n",
      "Epoch: 85, Train loss: 0.39456579089164734, Val loss: 0.410938024520874\n",
      "Epoch: 90, Train loss: 0.38985419273376465, Val loss: 0.38996052742004395\n",
      "Epoch: 95, Train loss: 0.39189326763153076, Val loss: 0.3935704231262207\n",
      "Epoch: 100, Train loss: 0.3890039622783661, Val loss: 0.3748772144317627\n"
     ]
    }
   ],
   "source": [
    "target = 0\n",
    "qm9_dataset.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_dataset.data.y = (qm9_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "num_features = qm9_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_dataset[0].edge_attr.shape[1]\n",
    "model = GINENet(num_features, dim_h, edge_attr).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GIN_0_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c743fd8-611f-4636-bed8-14dc2ffbf31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/1724566235.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"GIN_0_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for GIN: 0.29048705101013184\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = GINENet(num_features, dim_h, edge_attr).to(device)\n",
    "model.load_state_dict(torch.load(\"GIN_0_model.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "gin_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ee1c795-fc35-4e3d-9381-b63f29f96d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.7835645079612732, Val loss: 1.0414453744888306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m gin_best_loss, gin_train_loss, gin_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGIN_0_model_clo.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 29\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     28\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m train_loss[epoch] \u001b[38;5;241m=\u001b[39m epoch_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m val_loss[epoch] \u001b[38;5;241m=\u001b[39m v_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[157], line 40\u001b[0m, in \u001b[0;36mGINENet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m h1 \u001b[38;5;241m=\u001b[39m global_add_pool(h1, batch)\n\u001b[1;32m     39\u001b[0m h2 \u001b[38;5;241m=\u001b[39m global_add_pool(h2, batch)\n\u001b[0;32m---> 40\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_add_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Concatenate pooled features and pass through final linear layers\u001b[39;00m\n\u001b[1;32m     43\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((h1, h2, h3), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/pool/glob.py:34\u001b[0m, in \u001b[0;36mglobal_add_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target = 0\n",
    "qm9_clo_dataset.data.y = torch.Tensor(y_target[target])\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_clo_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_clo_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_clo_dataset.data.y = (qm9_clo_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_clo_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_clo_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_clo_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "num_features = qm9_clo_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_clo_dataset[0].edge_attr.shape[1]\n",
    "model = GINENet(num_features, dim_h, edge_attr).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GIN_0_model_clo.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d31d5-d855-4454-8774-b1b1e0c907d7",
   "metadata": {},
   "source": [
    "# Fix Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "23a55bc3-6200-4427-9c2a-5736e0f6863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9 = QM9(root='dataset/QM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2e316032-e3bf-450f-bc92-e320c758af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
    "qm9.data.y = torch.Tensor(y_target[0])\n",
    "\n",
    "qm9 = qm9.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "012a1518-5bba-4732-9887-d15c8166347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 30000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9.data.y[0:train_index].mean()\n",
    "data_std = qm9.data.y[0:train_index].std()\n",
    "\n",
    "qm9.data.y = (qm9.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed399508-b43c-45d6-883d-7b9b201e0127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ed2b538c-9122-46ba-bb88-31ff588410da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def pyg_to_networkx_with_attributes(data):\n",
    "    # Convert PyG data to NetworkX graph, retaining node attributes\n",
    "    G = to_networkx(data, node_attrs=['x', 'pos', 'z'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    # Add global attributes manually (if any exist)\n",
    "    G.graph['y'] = data.y\n",
    "    G.graph['smiles'] = data.smiles\n",
    "    G.graph['name'] = data.name\n",
    "    G.graph['idx'] = data.idx\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fa8cbbcd-c9c8-4250-b824-2d4193bef849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_betweenness_centrality(data):\n",
    "    # Convert PyG data to NetworkX graph, preserving attributes\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute node betweenness centrality\n",
    "    node_betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "    node_centrality_values = list(node_betweenness.values())\n",
    "    node_centrality_tensor = torch.tensor(node_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append node betweenness centrality to node features\n",
    "    data.x = torch.cat([data.x, node_centrality_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "    edge_centrality_values = [edge_betweenness[(u, v)] for u, v in G.edges()]\n",
    "    edge_centrality_tensor = torch.tensor(edge_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append edge betweenness centrality to edge features\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7a17df6f-8a4d-4889-93e7-b3b09b2f855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_data = qm9[:data_size]\n",
    "qm9_centrality = copy.deepcopy(qm9_data)\n",
    "qm9_bet = []\n",
    "for data in qm9_centrality:\n",
    "    data_bet = add_betweenness_centrality(data)\n",
    "    qm9_bet.append(data_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6b799061-6c4d-4e97-b494-16cf5d741db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_bet[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_bet[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_bet[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dddb60d1-6cb0-405c-b2c7-d2b587e78d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_bet[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_bet[0].edge_attr.shape[1]\n",
    "model = GINENet(num_features, dim_h, edge_attr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c4fc5563-f64d-4fc0-8abb-a5e2d3193b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.4425533711910248, Val loss: 0.44334885478019714\n",
      "Epoch: 10, Train loss: 0.4316164553165436, Val loss: 0.43632057309150696\n",
      "Epoch: 15, Train loss: 0.4227544665336609, Val loss: 0.42684265971183777\n",
      "Epoch: 20, Train loss: 0.41685619950294495, Val loss: 0.40088707208633423\n",
      "Epoch: 25, Train loss: 0.4120086133480072, Val loss: 0.4001424312591553\n",
      "Epoch: 30, Train loss: 0.4075751006603241, Val loss: 0.3974928855895996\n",
      "Epoch: 35, Train loss: 0.4040742516517639, Val loss: 0.3979135751724243\n",
      "Epoch: 40, Train loss: 0.3998916447162628, Val loss: 0.4012698233127594\n",
      "Epoch: 45, Train loss: 0.398292601108551, Val loss: 0.39024898409843445\n",
      "Epoch: 50, Train loss: 0.39782747626304626, Val loss: 0.3999740779399872\n",
      "Epoch: 55, Train loss: 0.39592859148979187, Val loss: 0.38609248399734497\n",
      "Epoch: 60, Train loss: 0.38877934217453003, Val loss: 0.39995473623275757\n",
      "Epoch: 65, Train loss: 0.39246419072151184, Val loss: 0.38175129890441895\n",
      "Epoch: 70, Train loss: 0.3892141282558441, Val loss: 0.38838639855384827\n",
      "Epoch: 75, Train loss: 0.38553157448768616, Val loss: 0.3822791874408722\n",
      "Epoch: 80, Train loss: 0.38313761353492737, Val loss: 0.39620187878608704\n",
      "Epoch: 85, Train loss: 0.382597416639328, Val loss: 0.3810156583786011\n",
      "Epoch: 90, Train loss: 0.3810938894748688, Val loss: 0.3730538785457611\n",
      "Epoch: 95, Train loss: 0.378468781709671, Val loss: 0.37700292468070984\n",
      "Epoch: 100, Train loss: 0.3749369978904724, Val loss: 0.3850788176059723\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GIN_0_model_bet.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d0ba49ae-e997-4898-a2b3-8dcdda2fa1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/2409921137.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"GIN_0_model_bet.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for GIN: 0.30340367555618286\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = GINENet(num_features, dim_h, edge_attr).to(device)\n",
    "model.load_state_dict(torch.load(\"GIN_0_model_bet.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "gin_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "11fdf829-875d-47ae-abc5-54e95ca6d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network class with 3 GINConv layers and 2 linear layers\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, dim_h):\n",
    "        \"\"\"Initializing GIN class\n",
    "\n",
    "        Args:\n",
    "            dim_h (int): the dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(num_features, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())\n",
    "        )\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.lin1 = Linear(dim_h, dim_h)\n",
    "        self.lin2 = Linear(dim_h, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        # Node embeddings\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.relu()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.relu()\n",
    "        h = self.conv3(h, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h = global_add_pool(h, batch)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c6502df0-2773-4d28-876f-c98eef371917",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_bet[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_bet[0].edge_attr.shape[1]\n",
    "model = GIN(num_features, dim_h).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3b2c6472-9523-416d-a6ff-22b8808c4d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.4953996241092682, Val loss: 0.4647423326969147\n",
      "Epoch: 10, Train loss: 0.46984025835990906, Val loss: 0.4653249979019165\n",
      "Epoch: 15, Train loss: 0.4588661789894104, Val loss: 0.4172241687774658\n",
      "Epoch: 20, Train loss: 0.44611597061157227, Val loss: 0.42408013343811035\n",
      "Epoch: 25, Train loss: 0.4390184283256531, Val loss: 0.4167090356349945\n",
      "Epoch: 30, Train loss: 0.4352353513240814, Val loss: 0.3947773277759552\n",
      "Epoch: 35, Train loss: 0.43027421832084656, Val loss: 0.4067845940589905\n",
      "Epoch: 40, Train loss: 0.4220568835735321, Val loss: 0.426036536693573\n",
      "Epoch: 45, Train loss: 0.42082715034484863, Val loss: 0.3889799416065216\n",
      "Epoch: 50, Train loss: 0.41620951890945435, Val loss: 0.3973839581012726\n",
      "Epoch: 55, Train loss: 0.4133622646331787, Val loss: 0.3881092071533203\n",
      "Epoch: 60, Train loss: 0.4112772047519684, Val loss: 0.38772523403167725\n",
      "Epoch: 65, Train loss: 0.4093972444534302, Val loss: 0.3814649283885956\n",
      "Epoch: 70, Train loss: 0.4051569998264313, Val loss: 0.386212021112442\n",
      "Epoch: 75, Train loss: 0.40491312742233276, Val loss: 0.3896653354167938\n",
      "Epoch: 80, Train loss: 0.4027821719646454, Val loss: 0.38554829359054565\n",
      "Epoch: 85, Train loss: 0.3999471664428711, Val loss: 0.3857475519180298\n",
      "Epoch: 90, Train loss: 0.4006689190864563, Val loss: 0.38073381781578064\n",
      "Epoch: 95, Train loss: 0.3980525732040405, Val loss: 0.3761279881000519\n",
      "Epoch: 100, Train loss: 0.39577484130859375, Val loss: 0.3909608721733093\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GINsimple_0_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "db566dfb-1b2d-4f4a-85fb-48bf401660ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/671566738.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"GINsimple_0_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for GIN: 0.31369003653526306\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = GIN(num_features, dim_h).to(device)\n",
    "model.load_state_dict(torch.load(\"GINsimple_0_model.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "gin_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6affb34-d829-4b1d-9471-af25f02518d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b4dcb-4781-47cf-91f9-dc3b5d047737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b239b62-077e-42f6-b459-86e75c6e6b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e0ae8f6d-ac3b-4ac3-a4ee-0f3d5d75d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ab73f504-655c-4ffe-8b70-cfa10efc9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9 = QM9(root='dataset/QM9')\n",
    "qm9_dataset = qm9[:5000]\n",
    "\n",
    "y_target = pd.DataFrame(qm9_dataset.data.y.numpy())\n",
    "qm9_dataset.data.y = torch.Tensor(y_target[0])\n",
    "\n",
    "qm9_dataset = qm9_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b7261-4e7a-4eb8-9264-7488dd83c2a0",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ea0596a8-7322-425d-9dfe-d4f2a98f953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import VirtualNode\n",
    "import copy\n",
    "\n",
    "qm9_vn = copy.deepcopy(qm9_dataset)\n",
    "\n",
    "transform = VirtualNode()\n",
    "qm9_vn.transform = transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9e72a21d-80ee-4541-a029-e0b305e5dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def pyg_to_networkx_with_attributes(data):\n",
    "    # Convert PyG data to NetworkX graph, retaining node attributes\n",
    "    G = to_networkx(data, node_attrs=['x', 'pos', 'z'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    # Add global attributes manually (if any exist)\n",
    "    G.graph['y'] = data.y\n",
    "    G.graph['smiles'] = data.smiles\n",
    "    G.graph['name'] = data.name\n",
    "    G.graph['idx'] = data.idx\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5977e060-874c-459a-98c8-ad24988c6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_betweenness_centrality(data):\n",
    "    # Convert PyG data to NetworkX graph, preserving attributes\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute node betweenness centrality\n",
    "    node_betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "    node_centrality_values = list(node_betweenness.values())\n",
    "    node_centrality_tensor = torch.tensor(node_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append node betweenness centrality to node features\n",
    "    data.x = torch.cat([data.x, node_centrality_tensor], dim=-1)\n",
    "\n",
    "    # Compute edge betweenness centrality\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "    edge_centrality_values = [edge_betweenness[(u, v)] for u, v in G.edges()]\n",
    "    edge_centrality_tensor = torch.tensor(edge_centrality_values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Append edge betweenness centrality to edge features\n",
    "    if 'edge_attr' in data:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, edge_centrality_tensor], dim=-1)\n",
    "    else:\n",
    "        data.edge_attr = edge_centrality_tensor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "28ce637b-7503-4bca-bf5e-e877f411eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "qm9_centrality = copy.deepcopy(qm9_dataset)\n",
    "qm9_bet = []\n",
    "for data in qm9_centrality:\n",
    "    data_bet = add_betweenness_centrality(data)\n",
    "    qm9_bet.append(data_bet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d76c1-6de7-4277-bf62-a7015b418256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4376bc0e-9906-4e1e-ba5f-cd1d0d0f9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_vn.data.y[0:train_index].mean()\n",
    "data_std = qm9_vn.data.y[0:train_index].std()\n",
    "\n",
    "qm9_vn.data.y = (qm9_vn.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_vn[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_vn[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_vn[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "601c716f-e91e-47dd-8338-da7ad2731012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class GINENet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr):\n",
    "        super(GINENet, self).__init__()\n",
    "        \n",
    "        # Define GINE layers with the specified edge_dim\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        \n",
    "        # Define linear layers for classification or regression\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through GINE layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3df60963-02fe-42e1-add3-94134b89185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PNAConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg):\n",
    "        super(PNANet, self).__init__()\n",
    "        \n",
    "        # Define PNA layers with specified aggregators, scalers, and degree tensor\n",
    "        self.conv1 = PNAConv(\n",
    "            in_channels=num_node_features,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv2 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv3 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        \n",
    "        # Define linear layers for final graph-level output\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through PNA layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c97e6fca-fdb3-4925-9781-4e803513d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
    "        model (nn.Module): GNN model to train on\n",
    "        loss (nn.functional): loss function to use during training\n",
    "        optimizer (torch.optim): optimizer during training\n",
    "\n",
    "    Returns:\n",
    "        float: training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        current_loss += l / len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model\n",
    "\n",
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): validation set in batches\n",
    "        model (nn.Module): current trained model\n",
    "        loss (nn.functional): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "    return val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): test dataset\n",
    "        model (nn.Module): trained model\n",
    "\n",
    "    Returns:\n",
    "        float: test loss\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # NOTE\n",
    "        # out = out.view(d.y.size())\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b582f0ee-e4a7-4aa6-ab38-255241e8726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    \"\"\"Training over all epochs\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs to train for\n",
    "        model (nn.Module): the current model\n",
    "        train_loader (DataLoader): training data in batches\n",
    "        val_loader (DataLoader): validation data in batches\n",
    "        path (string): path to save the best model\n",
    "\n",
    "    Returns:\n",
    "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.L1Loss()\n",
    "\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loader, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "        train_loss[epoch] = epoch_loss.detach().cpu().numpy()\n",
    "        val_loss[epoch] = v_loss.detach().cpu().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if epoch % 2 == 0:\n",
    "            print(\n",
    "                \"Epoch: \"\n",
    "                + str(epoch)\n",
    "                + \", Train loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return best_loss, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8a8f1c4c-1c05-4e39-ae63-a3a52cb79ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_vn[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_vn[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5e88fd41-5a64-4e49-90b9-bd94e630145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_vn[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_vn[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ba598436-c737-41bb-9a35-ffbd21a338bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.7384793758392334, Val loss: 0.6605138182640076\n",
      "Epoch: 4, Train loss: 0.6063035130500793, Val loss: 0.5809478163719177\n",
      "Epoch: 6, Train loss: 0.5589114427566528, Val loss: 0.5237354040145874\n",
      "Epoch: 8, Train loss: 0.5486401319503784, Val loss: 0.5280663371086121\n",
      "Epoch: 10, Train loss: 0.549461841583252, Val loss: 0.5279342532157898\n",
      "Epoch: 12, Train loss: 0.5425981283187866, Val loss: 0.5196453332901001\n",
      "Epoch: 14, Train loss: 0.5179474949836731, Val loss: 0.5465250015258789\n",
      "Epoch: 16, Train loss: 0.5011120438575745, Val loss: 0.47835907340049744\n",
      "Epoch: 18, Train loss: 0.5008600950241089, Val loss: 0.4659021198749542\n",
      "Epoch: 20, Train loss: 0.5041476488113403, Val loss: 0.48254144191741943\n",
      "Epoch: 22, Train loss: 0.4931771755218506, Val loss: 0.45759081840515137\n",
      "Epoch: 24, Train loss: 0.4926849901676178, Val loss: 0.4597529172897339\n",
      "Epoch: 26, Train loss: 0.4816162884235382, Val loss: 0.4646759033203125\n",
      "Epoch: 28, Train loss: 0.4744998812675476, Val loss: 0.45399782061576843\n",
      "Epoch: 30, Train loss: 0.4839800298213959, Val loss: 0.4549737870693207\n",
      "Epoch: 32, Train loss: 0.47806766629219055, Val loss: 0.4604285955429077\n",
      "Epoch: 34, Train loss: 0.46402403712272644, Val loss: 0.45829567313194275\n",
      "Epoch: 36, Train loss: 0.4560357928276062, Val loss: 0.43449532985687256\n",
      "Epoch: 38, Train loss: 0.4584978222846985, Val loss: 0.44415798783302307\n",
      "Epoch: 40, Train loss: 0.44228658080101013, Val loss: 0.4581582248210907\n",
      "Epoch: 42, Train loss: 0.45287320017814636, Val loss: 0.4795680642127991\n",
      "Epoch: 44, Train loss: 0.44954442977905273, Val loss: 0.4265892505645752\n",
      "Epoch: 46, Train loss: 0.43776339292526245, Val loss: 0.437309205532074\n",
      "Epoch: 48, Train loss: 0.4406234323978424, Val loss: 0.5089103579521179\n",
      "Epoch: 50, Train loss: 0.44277042150497437, Val loss: 0.442268431186676\n",
      "Epoch: 52, Train loss: 0.43812695145606995, Val loss: 0.416164368391037\n",
      "Epoch: 54, Train loss: 0.43945157527923584, Val loss: 0.4343717098236084\n",
      "Epoch: 56, Train loss: 0.4340100586414337, Val loss: 0.42280134558677673\n",
      "Epoch: 58, Train loss: 0.43187451362609863, Val loss: 0.44046884775161743\n",
      "Epoch: 60, Train loss: 0.43097198009490967, Val loss: 0.411386638879776\n",
      "Epoch: 62, Train loss: 0.4417065680027008, Val loss: 0.42806607484817505\n",
      "Epoch: 64, Train loss: 0.4173647463321686, Val loss: 0.49954187870025635\n",
      "Epoch: 66, Train loss: 0.43172770738601685, Val loss: 0.4736212491989136\n",
      "Epoch: 68, Train loss: 0.42167478799819946, Val loss: 0.4013039767742157\n",
      "Epoch: 70, Train loss: 0.4121140241622925, Val loss: 0.39977362751960754\n",
      "Epoch: 72, Train loss: 0.4276406466960907, Val loss: 0.3843633830547333\n",
      "Epoch: 74, Train loss: 0.4124791622161865, Val loss: 0.41452842950820923\n",
      "Epoch: 76, Train loss: 0.4195871651172638, Val loss: 0.40782904624938965\n",
      "Epoch: 78, Train loss: 0.40775758028030396, Val loss: 0.40484654903411865\n",
      "Epoch: 80, Train loss: 0.4055226445198059, Val loss: 0.38969385623931885\n",
      "Epoch: 82, Train loss: 0.3982291519641876, Val loss: 0.39613139629364014\n",
      "Epoch: 84, Train loss: 0.4011523723602295, Val loss: 0.3949512839317322\n",
      "Epoch: 86, Train loss: 0.40426164865493774, Val loss: 0.3931276798248291\n",
      "Epoch: 88, Train loss: 0.39516621828079224, Val loss: 0.3991810083389282\n",
      "Epoch: 90, Train loss: 0.41118109226226807, Val loss: 0.4275716245174408\n",
      "Epoch: 92, Train loss: 0.3948938250541687, Val loss: 0.39908063411712646\n",
      "Epoch: 94, Train loss: 0.390148788690567, Val loss: 0.3843926787376404\n",
      "Epoch: 96, Train loss: 0.3875100612640381, Val loss: 0.40570011734962463\n",
      "Epoch: 98, Train loss: 0.3924099802970886, Val loss: 0.3718090057373047\n",
      "Epoch: 100, Train loss: 0.38253358006477356, Val loss: 0.3882473409175873\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_vn.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "80611114-c9ff-473f-b0fb-3fec2c617416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/3144575080.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_vn.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.4123680591583252\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_vn.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961112ef-f9af-4191-9e21-b62dcd5469f3",
   "metadata": {},
   "source": [
    "## Bet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "393ba9c5-7dc8-4057-bdd6-e72c24073ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQM9Dataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomQM9Dataset, self).__init__()\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        \n",
    "qm9_bet_dataset = CustomQM9Dataset(qm9_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a7bcca20-70e1-4200-ba1b-246656117cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16, 12], edge_index=[2, 32], edge_attr=[32, 5], y=[1], pos=[16, 3], z=[16], smiles='[H]OC1=C(N([H])[H])C([H])=C(C([H])([H])[H])N1[H]', name='gdb_4492', idx=[1])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_bet_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6d0cba69-7864-48d5-b40f-74d99d8b953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16, 12], edge_index=[2, 32], edge_attr=[32, 5], y=[1], pos=[16, 3], z=[16], smiles='[H]OC1=C(N([H])[H])C([H])=C(C([H])([H])[H])N1[H]', name='gdb_4492', idx=[1])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_bet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a8eaf8c8-746c-47d9-8262-a2d883e41fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_bet_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_bet_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_bet_dataset.data.y = (qm9_bet_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_bet_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_bet_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_bet_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_bet_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_bet_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9e92d6c7-9b38-4dcb-91a8-d9953b61c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.6587304472923279, Val loss: 0.5765650868415833\n",
      "Epoch: 4, Train loss: 0.583828866481781, Val loss: 0.5183671712875366\n",
      "Epoch: 6, Train loss: 0.5375897288322449, Val loss: 0.48116886615753174\n",
      "Epoch: 8, Train loss: 0.5034983158111572, Val loss: 0.5219138264656067\n",
      "Epoch: 10, Train loss: 0.5191171169281006, Val loss: 0.45960476994514465\n",
      "Epoch: 12, Train loss: 0.4951198697090149, Val loss: 0.45158034563064575\n",
      "Epoch: 14, Train loss: 0.4819163382053375, Val loss: 0.4444285035133362\n",
      "Epoch: 16, Train loss: 0.49036213755607605, Val loss: 0.44865548610687256\n",
      "Epoch: 18, Train loss: 0.48305392265319824, Val loss: 0.45511487126350403\n",
      "Epoch: 20, Train loss: 0.4717722535133362, Val loss: 0.4234985411167145\n",
      "Epoch: 22, Train loss: 0.46605774760246277, Val loss: 0.4434279799461365\n",
      "Epoch: 24, Train loss: 0.4499996304512024, Val loss: 0.4987345337867737\n",
      "Epoch: 26, Train loss: 0.45014825463294983, Val loss: 0.42489251494407654\n",
      "Epoch: 28, Train loss: 0.44707202911376953, Val loss: 0.42595210671424866\n",
      "Epoch: 30, Train loss: 0.442355751991272, Val loss: 0.4208672344684601\n",
      "Epoch: 32, Train loss: 0.44103357195854187, Val loss: 0.41847294569015503\n",
      "Epoch: 34, Train loss: 0.430797815322876, Val loss: 0.3834829032421112\n",
      "Epoch: 36, Train loss: 0.4204290807247162, Val loss: 0.430942565202713\n",
      "Epoch: 38, Train loss: 0.4242398142814636, Val loss: 0.40997254848480225\n",
      "Epoch: 40, Train loss: 0.4202386140823364, Val loss: 0.39597269892692566\n",
      "Epoch: 42, Train loss: 0.42907723784446716, Val loss: 0.4094885289669037\n",
      "Epoch: 44, Train loss: 0.40562674403190613, Val loss: 0.3660328984260559\n",
      "Epoch: 46, Train loss: 0.40514475107192993, Val loss: 0.38547301292419434\n",
      "Epoch: 48, Train loss: 0.39471960067749023, Val loss: 0.38084155321121216\n",
      "Epoch: 50, Train loss: 0.408766508102417, Val loss: 0.4001011848449707\n",
      "Epoch: 52, Train loss: 0.388742595911026, Val loss: 0.34562838077545166\n",
      "Epoch: 54, Train loss: 0.3792895972728729, Val loss: 0.35783684253692627\n",
      "Epoch: 56, Train loss: 0.38506650924682617, Val loss: 0.36373063921928406\n",
      "Epoch: 58, Train loss: 0.39076730608940125, Val loss: 0.3732495605945587\n",
      "Epoch: 60, Train loss: 0.3769383728504181, Val loss: 0.3832218050956726\n",
      "Epoch: 62, Train loss: 0.3780320882797241, Val loss: 0.37914103269577026\n",
      "Epoch: 64, Train loss: 0.37950536608695984, Val loss: 0.35909032821655273\n",
      "Epoch: 66, Train loss: 0.3659202754497528, Val loss: 0.3591327965259552\n",
      "Epoch: 68, Train loss: 0.3617236614227295, Val loss: 0.37201249599456787\n",
      "Epoch: 70, Train loss: 0.3624151349067688, Val loss: 0.35230785608291626\n",
      "Epoch: 72, Train loss: 0.3649298846721649, Val loss: 0.38517358899116516\n",
      "Epoch: 74, Train loss: 0.36847662925720215, Val loss: 0.3402070701122284\n",
      "Epoch: 76, Train loss: 0.3677787780761719, Val loss: 0.34526777267456055\n",
      "Epoch: 78, Train loss: 0.3804245591163635, Val loss: 0.351471871137619\n",
      "Epoch: 80, Train loss: 0.3634454607963562, Val loss: 0.3795489966869354\n",
      "Epoch: 82, Train loss: 0.3507414162158966, Val loss: 0.349081814289093\n",
      "Epoch: 84, Train loss: 0.35124075412750244, Val loss: 0.3254915773868561\n",
      "Epoch: 86, Train loss: 0.36196738481521606, Val loss: 0.3493024408817291\n",
      "Epoch: 88, Train loss: 0.3441442847251892, Val loss: 0.36085212230682373\n",
      "Epoch: 90, Train loss: 0.3428940176963806, Val loss: 0.34423795342445374\n",
      "Epoch: 92, Train loss: 0.34257298707962036, Val loss: 0.3392760753631592\n",
      "Epoch: 94, Train loss: 0.33962664008140564, Val loss: 0.33550432324409485\n",
      "Epoch: 96, Train loss: 0.34176790714263916, Val loss: 0.34582817554473877\n",
      "Epoch: 98, Train loss: 0.33872467279434204, Val loss: 0.325813353061676\n",
      "Epoch: 100, Train loss: 0.3343064486980438, Val loss: 0.3670530319213867\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_bet[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_bet[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_bet.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3d896625-5033-426c-bc6b-312c22add844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/1572355086.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_bet.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.38897988200187683\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_bet.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2c75c3be-c159-4105-83a9-81c98755c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_dataset.data.y = (qm9_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4b21fc02-525b-48af-a35a-23295287e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.6711087226867676, Val loss: 0.6063773036003113\n",
      "Epoch: 4, Train loss: 0.6090232133865356, Val loss: 0.546333909034729\n",
      "Epoch: 6, Train loss: 0.5833888053894043, Val loss: 0.5302543640136719\n",
      "Epoch: 8, Train loss: 0.5726338028907776, Val loss: 0.5908280611038208\n",
      "Epoch: 10, Train loss: 0.5546010732650757, Val loss: 0.5251883268356323\n",
      "Epoch: 12, Train loss: 0.5343540906906128, Val loss: 0.49096959829330444\n",
      "Epoch: 14, Train loss: 0.5265180468559265, Val loss: 0.5319472551345825\n",
      "Epoch: 16, Train loss: 0.5307227969169617, Val loss: 0.5027675032615662\n",
      "Epoch: 18, Train loss: 0.5154920220375061, Val loss: 0.4776037931442261\n",
      "Epoch: 20, Train loss: 0.5007166266441345, Val loss: 0.5244244337081909\n",
      "Epoch: 22, Train loss: 0.5101432204246521, Val loss: 0.5580991506576538\n",
      "Epoch: 24, Train loss: 0.49939224123954773, Val loss: 0.4896053075790405\n",
      "Epoch: 26, Train loss: 0.48717933893203735, Val loss: 0.4847613573074341\n",
      "Epoch: 28, Train loss: 0.48831191658973694, Val loss: 0.48888882994651794\n",
      "Epoch: 30, Train loss: 0.482563853263855, Val loss: 0.44739386439323425\n",
      "Epoch: 32, Train loss: 0.4736810028553009, Val loss: 0.4899660348892212\n",
      "Epoch: 34, Train loss: 0.46854957938194275, Val loss: 0.4966825842857361\n",
      "Epoch: 36, Train loss: 0.46362099051475525, Val loss: 0.46195468306541443\n",
      "Epoch: 38, Train loss: 0.46312758326530457, Val loss: 0.47995123267173767\n",
      "Epoch: 40, Train loss: 0.4571489095687866, Val loss: 0.44259926676750183\n",
      "Epoch: 42, Train loss: 0.44968363642692566, Val loss: 0.41836991906166077\n",
      "Epoch: 44, Train loss: 0.45560282468795776, Val loss: 0.44355207681655884\n",
      "Epoch: 46, Train loss: 0.4480189085006714, Val loss: 0.4276750087738037\n",
      "Epoch: 48, Train loss: 0.4410213828086853, Val loss: 0.4445192515850067\n",
      "Epoch: 50, Train loss: 0.4373883903026581, Val loss: 0.47111329436302185\n",
      "Epoch: 52, Train loss: 0.4379785656929016, Val loss: 0.4964373707771301\n",
      "Epoch: 54, Train loss: 0.43293750286102295, Val loss: 0.4138405919075012\n",
      "Epoch: 56, Train loss: 0.43128496408462524, Val loss: 0.42339038848876953\n",
      "Epoch: 58, Train loss: 0.4283908009529114, Val loss: 0.39472275972366333\n",
      "Epoch: 60, Train loss: 0.4266587495803833, Val loss: 0.4184975028038025\n",
      "Epoch: 62, Train loss: 0.41981041431427, Val loss: 0.41774916648864746\n",
      "Epoch: 64, Train loss: 0.40694406628608704, Val loss: 0.40044185519218445\n",
      "Epoch: 66, Train loss: 0.40744003653526306, Val loss: 0.4572131633758545\n",
      "Epoch: 68, Train loss: 0.4092421233654022, Val loss: 0.39275020360946655\n",
      "Epoch: 70, Train loss: 0.4131903648376465, Val loss: 0.4288266599178314\n",
      "Epoch: 72, Train loss: 0.39830732345581055, Val loss: 0.4248112738132477\n",
      "Epoch: 74, Train loss: 0.3940427005290985, Val loss: 0.3983134627342224\n",
      "Epoch: 76, Train loss: 0.40500470995903015, Val loss: 0.3913496136665344\n",
      "Epoch: 78, Train loss: 0.39058059453964233, Val loss: 0.3773117959499359\n",
      "Epoch: 80, Train loss: 0.4031432271003723, Val loss: 0.4289701581001282\n",
      "Epoch: 82, Train loss: 0.3915644586086273, Val loss: 0.41729968786239624\n",
      "Epoch: 84, Train loss: 0.3952493965625763, Val loss: 0.37969622015953064\n",
      "Epoch: 86, Train loss: 0.38697943091392517, Val loss: 0.36556515097618103\n",
      "Epoch: 88, Train loss: 0.38181132078170776, Val loss: 0.3859773576259613\n",
      "Epoch: 90, Train loss: 0.3877488076686859, Val loss: 0.3852097988128662\n",
      "Epoch: 92, Train loss: 0.37764737010002136, Val loss: 0.37637969851493835\n",
      "Epoch: 94, Train loss: 0.37806040048599243, Val loss: 0.41848069429397583\n",
      "Epoch: 96, Train loss: 0.37770986557006836, Val loss: 0.3672640323638916\n",
      "Epoch: 98, Train loss: 0.3708869516849518, Val loss: 0.3938848674297333\n",
      "Epoch: 100, Train loss: 0.38288193941116333, Val loss: 0.3957955241203308\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c25123a4-4842-461f-9ee0-bf0fe4b8eff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/1113277864.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.45471668243408203\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388544ba-0304-446d-9f31-9332d42ed2f6",
   "metadata": {},
   "source": [
    "# Deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "56b64b23-b69a-4b28-b892-f18a16c1bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centrality_to_node_features(data, centrality_measure='degree'):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if G.is_directed():\n",
    "            G = G.to_undirected()\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "\n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = list(centrality.values())\n",
    "    centrality_tensor = torch.tensor(centrality_values, dtype=torch.float).view(-1, 1)\n",
    "    centrality_tensor = (centrality_tensor - centrality_tensor.mean()) / (centrality_tensor.std() + 1e-8)\n",
    "    data.x = torch.cat([data.x, centrality_tensor], dim=-1)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "32d9eafb-8503-420e-bf1b-ecee5a26dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_deg = []\n",
    "for data in qm9_centrality:\n",
    "    data_deg = add_centrality_to_node_features(data, centrality_measure='degree')\n",
    "    qm9_deg.append(data_deg)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "696f4b22-3e69-4031-a618-30708671eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_deg_dataset = CustomQM9Dataset(qm9_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4adf9842-bd72-4e2a-8db4-61f6d5ef745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_deg_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_deg_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_deg_dataset.data.y = (qm9_deg_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_deg_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_deg_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_deg_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_deg_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_deg_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b9f10d32-ade2-4ca9-9d61-42fe5c012f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.6537501811981201, Val loss: 0.5716278553009033\n",
      "Epoch: 4, Train loss: 0.5661993026733398, Val loss: 0.5297939777374268\n",
      "Epoch: 6, Train loss: 0.538526713848114, Val loss: 0.5175751447677612\n",
      "Epoch: 8, Train loss: 0.5230590105056763, Val loss: 0.4853278398513794\n",
      "Epoch: 10, Train loss: 0.5258864164352417, Val loss: 0.4737209379673004\n",
      "Epoch: 12, Train loss: 0.507095992565155, Val loss: 0.4825216233730316\n",
      "Epoch: 14, Train loss: 0.48469388484954834, Val loss: 0.4710986912250519\n",
      "Epoch: 16, Train loss: 0.4969146251678467, Val loss: 0.47007662057876587\n",
      "Epoch: 18, Train loss: 0.486106276512146, Val loss: 0.4392777681350708\n",
      "Epoch: 20, Train loss: 0.47254419326782227, Val loss: 0.4269437789916992\n",
      "Epoch: 22, Train loss: 0.47093844413757324, Val loss: 0.4293811321258545\n",
      "Epoch: 24, Train loss: 0.4430338442325592, Val loss: 0.42953670024871826\n",
      "Epoch: 26, Train loss: 0.4539099335670471, Val loss: 0.49615585803985596\n",
      "Epoch: 28, Train loss: 0.46374017000198364, Val loss: 0.4228176176548004\n",
      "Epoch: 30, Train loss: 0.4429478347301483, Val loss: 0.405792236328125\n",
      "Epoch: 32, Train loss: 0.4353732168674469, Val loss: 0.40500694513320923\n",
      "Epoch: 34, Train loss: 0.4240373969078064, Val loss: 0.4244840741157532\n",
      "Epoch: 36, Train loss: 0.4333569407463074, Val loss: 0.4781583547592163\n",
      "Epoch: 38, Train loss: 0.4204607605934143, Val loss: 0.43010684847831726\n",
      "Epoch: 40, Train loss: 0.42458903789520264, Val loss: 0.3839012384414673\n",
      "Epoch: 42, Train loss: 0.4226539134979248, Val loss: 0.45713719725608826\n",
      "Epoch: 44, Train loss: 0.4045645594596863, Val loss: 0.37956273555755615\n",
      "Epoch: 46, Train loss: 0.3867955505847931, Val loss: 0.37863755226135254\n",
      "Epoch: 48, Train loss: 0.40006139874458313, Val loss: 0.39149320125579834\n",
      "Epoch: 50, Train loss: 0.40971699357032776, Val loss: 0.40675535798072815\n",
      "Epoch: 52, Train loss: 0.39547717571258545, Val loss: 0.3664025068283081\n",
      "Epoch: 54, Train loss: 0.39430657029151917, Val loss: 0.38021647930145264\n",
      "Epoch: 56, Train loss: 0.3837626576423645, Val loss: 0.3904678523540497\n",
      "Epoch: 58, Train loss: 0.3729507029056549, Val loss: 0.3597385287284851\n",
      "Epoch: 60, Train loss: 0.3938724398612976, Val loss: 0.3561457395553589\n",
      "Epoch: 62, Train loss: 0.38769692182540894, Val loss: 0.35587334632873535\n",
      "Epoch: 64, Train loss: 0.38904768228530884, Val loss: 0.357145756483078\n",
      "Epoch: 66, Train loss: 0.3850051760673523, Val loss: 0.36447641253471375\n",
      "Epoch: 68, Train loss: 0.36628279089927673, Val loss: 0.36304834485054016\n",
      "Epoch: 70, Train loss: 0.3748897910118103, Val loss: 0.34606292843818665\n",
      "Epoch: 72, Train loss: 0.3721778690814972, Val loss: 0.3403604328632355\n",
      "Epoch: 74, Train loss: 0.3709867000579834, Val loss: 0.3801526725292206\n",
      "Epoch: 76, Train loss: 0.3581833243370056, Val loss: 0.3720061480998993\n",
      "Epoch: 78, Train loss: 0.36732417345046997, Val loss: 0.3551390767097473\n",
      "Epoch: 80, Train loss: 0.36240750551223755, Val loss: 0.3686537742614746\n",
      "Epoch: 82, Train loss: 0.36326661705970764, Val loss: 0.3374330997467041\n",
      "Epoch: 84, Train loss: 0.35527822375297546, Val loss: 0.3668730556964874\n",
      "Epoch: 86, Train loss: 0.35001006722450256, Val loss: 0.3538661003112793\n",
      "Epoch: 88, Train loss: 0.34631335735321045, Val loss: 0.3620006740093231\n",
      "Epoch: 90, Train loss: 0.3620603680610657, Val loss: 0.34299105405807495\n",
      "Epoch: 92, Train loss: 0.3609558045864105, Val loss: 0.4055117964744568\n",
      "Epoch: 94, Train loss: 0.34988659620285034, Val loss: 0.3454754054546356\n",
      "Epoch: 96, Train loss: 0.35268500447273254, Val loss: 0.34860286116600037\n",
      "Epoch: 98, Train loss: 0.3453843891620636, Val loss: 0.33371374011039734\n",
      "Epoch: 100, Train loss: 0.35264644026756287, Val loss: 0.36094754934310913\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_deg_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_deg_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_deg.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "818cde6b-2325-4dfa-ba0a-ae9441d94fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/390849739.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_deg.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.3684897720813751\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_deg.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3deb9ffe-0e10-4634-bec3-80c9a9aa619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_clo = []\n",
    "for data in qm9_centrality:\n",
    "    data_clo = add_centrality_to_node_features(data, centrality_measure='closeness')\n",
    "    qm9_clo.append(data_clo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8d3be082-1fd5-4058-9160-5aa56209e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_clo_dataset = CustomQM9Dataset(qm9_clo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "23197056-1b72-47f7-b4ab-49a556737039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_clo_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_clo_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_clo_dataset.data.y = (qm9_clo_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_clo_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_clo_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_clo_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_clo_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_clo_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f858e164-0bcf-4ce3-a787-0407201fb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.6393109560012817, Val loss: 0.5559122562408447\n",
      "Epoch: 4, Train loss: 0.5645928382873535, Val loss: 0.5347018241882324\n",
      "Epoch: 6, Train loss: 0.5396029949188232, Val loss: 0.5057032108306885\n",
      "Epoch: 8, Train loss: 0.5147683620452881, Val loss: 0.46638962626457214\n",
      "Epoch: 10, Train loss: 0.49338993430137634, Val loss: 0.44360896944999695\n",
      "Epoch: 12, Train loss: 0.48792511224746704, Val loss: 0.4718412756919861\n",
      "Epoch: 14, Train loss: 0.47131863236427307, Val loss: 0.4388616979122162\n",
      "Epoch: 16, Train loss: 0.4629957377910614, Val loss: 0.40439149737358093\n",
      "Epoch: 18, Train loss: 0.46282726526260376, Val loss: 0.42124560475349426\n",
      "Epoch: 20, Train loss: 0.4612669348716736, Val loss: 0.42351841926574707\n",
      "Epoch: 22, Train loss: 0.4445327818393707, Val loss: 0.41041451692581177\n",
      "Epoch: 24, Train loss: 0.43824294209480286, Val loss: 0.45043596625328064\n",
      "Epoch: 26, Train loss: 0.4424562454223633, Val loss: 0.3897877335548401\n",
      "Epoch: 28, Train loss: 0.4349595010280609, Val loss: 0.4169524013996124\n",
      "Epoch: 30, Train loss: 0.4346945881843567, Val loss: 0.38825514912605286\n",
      "Epoch: 32, Train loss: 0.4273001253604889, Val loss: 0.4010990858078003\n",
      "Epoch: 34, Train loss: 0.42965978384017944, Val loss: 0.39581167697906494\n",
      "Epoch: 36, Train loss: 0.42041972279548645, Val loss: 0.3843863010406494\n",
      "Epoch: 38, Train loss: 0.43412071466445923, Val loss: 0.43980300426483154\n",
      "Epoch: 40, Train loss: 0.42020654678344727, Val loss: 0.41320788860321045\n",
      "Epoch: 42, Train loss: 0.41733476519584656, Val loss: 0.37390023469924927\n",
      "Epoch: 44, Train loss: 0.41387230157852173, Val loss: 0.3930096924304962\n",
      "Epoch: 46, Train loss: 0.4083314538002014, Val loss: 0.3758452832698822\n",
      "Epoch: 48, Train loss: 0.4019177556037903, Val loss: 0.3963609039783478\n",
      "Epoch: 50, Train loss: 0.4057370722293854, Val loss: 0.37396854162216187\n",
      "Epoch: 52, Train loss: 0.39728569984436035, Val loss: 0.3984062373638153\n",
      "Epoch: 54, Train loss: 0.40362250804901123, Val loss: 0.3960849940776825\n",
      "Epoch: 56, Train loss: 0.41011714935302734, Val loss: 0.39497727155685425\n",
      "Epoch: 58, Train loss: 0.39188334345817566, Val loss: 0.38822636008262634\n",
      "Epoch: 60, Train loss: 0.3933227062225342, Val loss: 0.37412968277931213\n",
      "Epoch: 62, Train loss: 0.39185014367103577, Val loss: 0.34762275218963623\n",
      "Epoch: 64, Train loss: 0.3825763463973999, Val loss: 0.3743537664413452\n",
      "Epoch: 66, Train loss: 0.3798721432685852, Val loss: 0.3798498809337616\n",
      "Epoch: 68, Train loss: 0.3786177337169647, Val loss: 0.374411404132843\n",
      "Epoch: 70, Train loss: 0.3750370144844055, Val loss: 0.36679601669311523\n",
      "Epoch: 72, Train loss: 0.37213677167892456, Val loss: 0.39039504528045654\n",
      "Epoch: 74, Train loss: 0.37543803453445435, Val loss: 0.36909452080726624\n",
      "Epoch: 76, Train loss: 0.3723275661468506, Val loss: 0.35022640228271484\n",
      "Epoch: 78, Train loss: 0.36582034826278687, Val loss: 0.3687770366668701\n",
      "Epoch: 80, Train loss: 0.37158337235450745, Val loss: 0.390180766582489\n",
      "Epoch: 82, Train loss: 0.3785558342933655, Val loss: 0.35247087478637695\n",
      "Epoch: 84, Train loss: 0.36832791566848755, Val loss: 0.37085551023483276\n",
      "Epoch: 86, Train loss: 0.3694928288459778, Val loss: 0.3578552007675171\n",
      "Epoch: 88, Train loss: 0.36312559247016907, Val loss: 0.3831549286842346\n",
      "Epoch: 90, Train loss: 0.3758045732975006, Val loss: 0.36269599199295044\n",
      "Epoch: 92, Train loss: 0.3615555465221405, Val loss: 0.3532678484916687\n",
      "Epoch: 94, Train loss: 0.35964539647102356, Val loss: 0.34440478682518005\n",
      "Epoch: 96, Train loss: 0.35405707359313965, Val loss: 0.36213910579681396\n",
      "Epoch: 98, Train loss: 0.3611525893211365, Val loss: 0.34362274408340454\n",
      "Epoch: 100, Train loss: 0.3470357656478882, Val loss: 0.36760619282722473\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_clo_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_clo_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_clo.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "671662b6-73ec-4145-a9c0-39cc856fc3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.383576899766922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/1217052971.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_clo.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_clo.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e2620aca-69eb-48ad-9f18-3c12f6bdc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_eig = []\n",
    "for data in qm9_centrality:\n",
    "    data_eig = add_centrality_to_node_features(data, centrality_measure='eigenvector')\n",
    "    qm9_eig.append(data_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "97a9da49-24b2-4be5-a51f-7a6148455815",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_eig_dataset = CustomQM9Dataset(qm9_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e1ac5f21-c16b-48c3-bc24-84ba84181acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_eig_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_eig_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_eig_dataset.data.y = (qm9_eig_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_eig_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_eig_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_eig_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_eig_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_eig_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a808f7bf-0e94-4014-bb4d-bdf879d3148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.6574724316596985, Val loss: 0.5758398771286011\n",
      "Epoch: 4, Train loss: 0.5719799995422363, Val loss: 0.5563930869102478\n",
      "Epoch: 6, Train loss: 0.532564640045166, Val loss: 0.506310760974884\n",
      "Epoch: 8, Train loss: 0.5254233479499817, Val loss: 0.5619834065437317\n",
      "Epoch: 10, Train loss: 0.5194610953330994, Val loss: 0.4729815423488617\n",
      "Epoch: 12, Train loss: 0.5063085556030273, Val loss: 0.4623737037181854\n",
      "Epoch: 14, Train loss: 0.4895370304584503, Val loss: 0.44933250546455383\n",
      "Epoch: 16, Train loss: 0.4817868173122406, Val loss: 0.46185553073883057\n",
      "Epoch: 18, Train loss: 0.46930715441703796, Val loss: 0.4319245517253876\n",
      "Epoch: 20, Train loss: 0.4580933153629303, Val loss: 0.43622368574142456\n",
      "Epoch: 22, Train loss: 0.4605036675930023, Val loss: 0.43310990929603577\n",
      "Epoch: 24, Train loss: 0.45734116435050964, Val loss: 0.4273054301738739\n",
      "Epoch: 26, Train loss: 0.46473824977874756, Val loss: 0.4969777464866638\n",
      "Epoch: 28, Train loss: 0.4536641240119934, Val loss: 0.4241826832294464\n",
      "Epoch: 30, Train loss: 0.4414679706096649, Val loss: 0.41587790846824646\n",
      "Epoch: 32, Train loss: 0.447703093290329, Val loss: 0.41350170969963074\n",
      "Epoch: 34, Train loss: 0.4235195815563202, Val loss: 0.42414021492004395\n",
      "Epoch: 36, Train loss: 0.4185296893119812, Val loss: 0.4143866002559662\n",
      "Epoch: 38, Train loss: 0.4333232045173645, Val loss: 0.4332078993320465\n",
      "Epoch: 40, Train loss: 0.4299105703830719, Val loss: 0.5177644491195679\n",
      "Epoch: 42, Train loss: 0.4252093732357025, Val loss: 0.3941798210144043\n",
      "Epoch: 44, Train loss: 0.4112960994243622, Val loss: 0.3752188980579376\n",
      "Epoch: 46, Train loss: 0.40063682198524475, Val loss: 0.3964357078075409\n",
      "Epoch: 48, Train loss: 0.40980055928230286, Val loss: 0.39631274342536926\n",
      "Epoch: 50, Train loss: 0.4107897877693176, Val loss: 0.3872910141944885\n",
      "Epoch: 52, Train loss: 0.39617034792900085, Val loss: 0.38952720165252686\n",
      "Epoch: 54, Train loss: 0.3877158761024475, Val loss: 0.4549653232097626\n",
      "Epoch: 56, Train loss: 0.4002245366573334, Val loss: 0.40521401166915894\n",
      "Epoch: 58, Train loss: 0.39225757122039795, Val loss: 0.3768134117126465\n",
      "Epoch: 60, Train loss: 0.38849395513534546, Val loss: 0.3774237632751465\n",
      "Epoch: 62, Train loss: 0.3851214349269867, Val loss: 0.37975040078163147\n",
      "Epoch: 64, Train loss: 0.3753146231174469, Val loss: 0.41480809450149536\n",
      "Epoch: 66, Train loss: 0.37611958384513855, Val loss: 0.3757021725177765\n",
      "Epoch: 68, Train loss: 0.38057616353034973, Val loss: 0.38113299012184143\n",
      "Epoch: 70, Train loss: 0.37374305725097656, Val loss: 0.36640164256095886\n",
      "Epoch: 72, Train loss: 0.386516273021698, Val loss: 0.36579445004463196\n",
      "Epoch: 74, Train loss: 0.3785881996154785, Val loss: 0.3586358428001404\n",
      "Epoch: 76, Train loss: 0.3677523136138916, Val loss: 0.35624104738235474\n",
      "Epoch: 78, Train loss: 0.37430283427238464, Val loss: 0.3636057376861572\n",
      "Epoch: 80, Train loss: 0.37518954277038574, Val loss: 0.36040955781936646\n",
      "Epoch: 82, Train loss: 0.3776366412639618, Val loss: 0.39279767870903015\n",
      "Epoch: 84, Train loss: 0.371338427066803, Val loss: 0.37716513872146606\n",
      "Epoch: 86, Train loss: 0.3610171377658844, Val loss: 0.38147032260894775\n",
      "Epoch: 88, Train loss: 0.3650628328323364, Val loss: 0.35862717032432556\n",
      "Epoch: 90, Train loss: 0.37402230501174927, Val loss: 0.359822541475296\n",
      "Epoch: 92, Train loss: 0.3666378855705261, Val loss: 0.3750475347042084\n",
      "Epoch: 94, Train loss: 0.37211114168167114, Val loss: 0.3619087338447571\n",
      "Epoch: 96, Train loss: 0.35677415132522583, Val loss: 0.37912195920944214\n",
      "Epoch: 98, Train loss: 0.36101630330085754, Val loss: 0.3656565248966217\n",
      "Epoch: 100, Train loss: 0.37538206577301025, Val loss: 0.3530326187610626\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_eig_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_eig_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_eig.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5b1d1ea5-eeb1-464c-9d46-16d54d7edc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 0.36430564522743225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/2637821783.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_eig.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_eig.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "80bfd6bc-33fc-4dd1-9859-29c5e204458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_subgraph_features(data, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "\n",
    "    # Initialize a list to store subgraph features for each node\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        # Extract the ego graph (subgraph) around the node\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        \n",
    "        # Example feature 1: Size of the subgraph (number of nodes)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "        \n",
    "        # Example feature 2: Average degree of the subgraph\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "        \n",
    "    # Convert the features to tensors and add them as node features\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Concatenate the new features to the existing node features\n",
    "    data.x = torch.cat([data.x, subgraph_sizes_tensor, subgraph_degrees_tensor], dim=-1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "bb9b1698-4ac0-4660-a8b4-7147225b2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_sub = copy.deepcopy(qm9_dataset)\n",
    "qm9_SE = []\n",
    "for data in qm9_sub:\n",
    "    data_sub = extract_local_subgraph_features_with_edges(data, radius=4)\n",
    "    qm9_SE.append(data_sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "739911fc-a159-401c-95a0-bff48575b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_higher_order_aggregation(data, radius=2):\n",
    "    \"\"\"Adds higher-order aggregation information based on neighborhood statistics.\"\"\"\n",
    "    G = to_networkx(data, to_undirected=True, node_attrs=['x'])\n",
    "    subgraph_means = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        # Get the subgraph within the specified radius\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        \n",
    "        # Ensure each node feature is a tensor\n",
    "        subgraph_features = []\n",
    "        for n in subgraph.nodes():\n",
    "            feature = G.nodes[n]['x']\n",
    "            feature_tensor = torch.tensor(feature, dtype=torch.float) if isinstance(feature, list) else feature\n",
    "            subgraph_features.append(feature_tensor)\n",
    "        \n",
    "        # Stack the subgraph features and calculate the mean\n",
    "        subgraph_features = torch.stack(subgraph_features)\n",
    "        subgraph_mean = subgraph_features.mean(dim=0)\n",
    "        subgraph_means.append(subgraph_mean)\n",
    "\n",
    "    # Concatenate the aggregated features with the original node features\n",
    "    subgraph_means_tensor = torch.stack(subgraph_means)\n",
    "    data.x = torch.cat([data.x, subgraph_means_tensor], dim=-1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply the transformation to the dataset\n",
    "qm9_higher_order = [add_higher_order_aggregation(data) for data in qm9_sub]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1d712417-40b7-4d51-93bb-616882aca2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_HO_dataset = CustomQM9Dataset(qm9_higher_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "38960a1a-6dd5-4911-a4da-9c48a3217ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_SE_dataset = CustomQM9Dataset(qm9_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "2e64edd5-c641-4df7-b034-2cc42fa5c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_HO_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_HO_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_HO_dataset.data.y = (qm9_HO_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_HO_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_HO_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_HO_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_HO_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_HO_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a6aa03b4-e494-4861-82bd-88e1f5b57874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.80887770652771, Val loss: 0.8398736715316772\n",
      "Epoch: 4, Train loss: 0.8001496195793152, Val loss: 0.8321776390075684\n",
      "Epoch: 6, Train loss: 0.7987918853759766, Val loss: 0.8343009352684021\n",
      "Epoch: 8, Train loss: 0.7973417639732361, Val loss: 0.8334730863571167\n",
      "Epoch: 10, Train loss: 0.7962929606437683, Val loss: 0.8378956317901611\n",
      "Epoch: 12, Train loss: 0.7957975268363953, Val loss: 0.8349051475524902\n",
      "Epoch: 14, Train loss: 0.7964848279953003, Val loss: 0.8340446949005127\n",
      "Epoch: 16, Train loss: 0.795705258846283, Val loss: 0.834448516368866\n",
      "Epoch: 18, Train loss: 0.7968242168426514, Val loss: 0.8355870246887207\n",
      "Epoch: 20, Train loss: 0.7952485084533691, Val loss: 0.8312394618988037\n",
      "Epoch: 22, Train loss: 0.7959997057914734, Val loss: 0.8321973085403442\n",
      "Epoch: 24, Train loss: 0.794975221157074, Val loss: 0.8321433663368225\n",
      "Epoch: 26, Train loss: 0.7941954731941223, Val loss: 0.8289918303489685\n",
      "Epoch: 28, Train loss: 0.7955598831176758, Val loss: 0.8308260440826416\n",
      "Epoch: 30, Train loss: 0.7956541180610657, Val loss: 0.8343716859817505\n",
      "Epoch: 32, Train loss: 0.7942659854888916, Val loss: 0.8332498669624329\n",
      "Epoch: 34, Train loss: 0.7942292094230652, Val loss: 0.83116614818573\n",
      "Epoch: 36, Train loss: 0.7952575087547302, Val loss: 0.8336942791938782\n",
      "Epoch: 38, Train loss: 0.7935879826545715, Val loss: 0.8306231498718262\n",
      "Epoch: 40, Train loss: 0.7937279343605042, Val loss: 0.8306036591529846\n",
      "Epoch: 42, Train loss: 0.795555591583252, Val loss: 0.8331257104873657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[322], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pna_best_loss, pna_train_loss, pna_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNA_0_model_HO.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[216], line 27\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     25\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m v_loss\n\u001b[1;32m     26\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), path)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     28\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:169\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[0;32m--> 169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m \u001b[43mget_incs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    172\u001b[0m             value \u001b[38;5;241m+\u001b[39m inc\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n\u001b[1;32m    174\u001b[0m         ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:321\u001b[0m, in \u001b[0;36mget_incs\u001b[0;34m(key, values, data_list, stores)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    320\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 321\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    322\u001b[0m         data\u001b[38;5;241m.\u001b[39m__inc__(key, value, store)\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    324\u001b[0m     ]\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    326\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/collate.py:322\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    320\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    321\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 322\u001b[0m         \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inc__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    324\u001b[0m     ]\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    326\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/data.py:658\u001b[0m, in \u001b[0;36mData.__inc__\u001b[0;34m(self, key, value, *args, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/data.py:614\u001b[0m, in \u001b[0;36mData.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/data.py:185\u001b[0m, in \u001b[0;36mBaseData.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of nodes in the graph.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    You will be given a warning that requests you to do so.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([v\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_stores])\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/data.py:185\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of nodes in the graph.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    You will be given a warning that requests you to do so.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_stores])\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/storage.py:425\u001b[0m, in \u001b[0;36mNodeStorage.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m N_KEYS:\n\u001b[0;32m--> 425\u001b[0m         cat_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__cat_dim__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39msize(cat_dim)\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m N_KEYS:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/data.py:647\u001b[0m, in \u001b[0;36mData.__cat_dim__\u001b[0;34m(self, key, value, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__cat_dim__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Any, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madj\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/sparse.py:140\u001b[0m, in \u001b[0;36mis_sparse\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_sparse\u001b[39m(src: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns :obj:`True` if the input :obj:`src` is of type\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    :class:`torch.sparse.Tensor` (in any sparse layout) or of type\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    :class:`torch_sparse.SparseTensor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m        src (Any): The input object to be checked.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_torch_sparse_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(src, SparseTensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/sparse.py:124\u001b[0m, in \u001b[0;36mis_torch_sparse_tensor\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_csr:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_PT112\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_csc):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_HO_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_HO_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_HO.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f5cad66c-4114-447d-855a-2eecd120ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.8013623356819153, Val loss: 0.8386300802230835\n",
      "Epoch: 4, Train loss: 0.7994137406349182, Val loss: 0.8310486674308777\n",
      "Epoch: 6, Train loss: 0.7961886525154114, Val loss: 0.8380690813064575\n",
      "Epoch: 8, Train loss: 0.7950918674468994, Val loss: 0.834450364112854\n",
      "Epoch: 10, Train loss: 0.7949660420417786, Val loss: 0.8336603045463562\n",
      "Epoch: 12, Train loss: 0.7950272560119629, Val loss: 0.8317904472351074\n",
      "Epoch: 14, Train loss: 0.796575129032135, Val loss: 0.8325912356376648\n",
      "Epoch: 16, Train loss: 0.7921549081802368, Val loss: 0.8310779333114624\n",
      "Epoch: 18, Train loss: 0.7916917204856873, Val loss: 0.8376547694206238\n",
      "Epoch: 20, Train loss: 0.7950850129127502, Val loss: 0.8332239985466003\n",
      "Epoch: 22, Train loss: 0.7932614684104919, Val loss: 0.8298947811126709\n",
      "Epoch: 24, Train loss: 0.7915077805519104, Val loss: 0.8310540914535522\n",
      "Epoch: 26, Train loss: 0.7924097180366516, Val loss: 0.8337491750717163\n",
      "Epoch: 28, Train loss: 0.792404294013977, Val loss: 0.8303728103637695\n",
      "Epoch: 30, Train loss: 0.7921820878982544, Val loss: 0.8296867609024048\n",
      "Epoch: 32, Train loss: 0.792961061000824, Val loss: 0.8352400064468384\n",
      "Epoch: 34, Train loss: 0.7933835387229919, Val loss: 0.8411014676094055\n",
      "Epoch: 36, Train loss: 0.7946513295173645, Val loss: 0.8331665396690369\n",
      "Epoch: 38, Train loss: 0.7911304235458374, Val loss: 0.8320515751838684\n",
      "Epoch: 40, Train loss: 0.7925861477851868, Val loss: 0.8346759676933289\n",
      "Epoch: 42, Train loss: 0.7945181727409363, Val loss: 0.8419528603553772\n",
      "Epoch: 44, Train loss: 0.7944372296333313, Val loss: 0.833935558795929\n",
      "Epoch: 46, Train loss: 0.793085515499115, Val loss: 0.8361355662345886\n",
      "Epoch: 48, Train loss: 0.7945454120635986, Val loss: 0.8315781354904175\n",
      "Epoch: 50, Train loss: 0.7927295565605164, Val loss: 0.8326647281646729\n",
      "Epoch: 52, Train loss: 0.7932004332542419, Val loss: 0.8341070413589478\n",
      "Epoch: 54, Train loss: 0.7922926545143127, Val loss: 0.8329535126686096\n",
      "Epoch: 56, Train loss: 0.7908538579940796, Val loss: 0.8353545069694519\n",
      "Epoch: 58, Train loss: 0.791149377822876, Val loss: 0.8343154191970825\n",
      "Epoch: 60, Train loss: 0.7918223142623901, Val loss: 0.8308514356613159\n",
      "Epoch: 62, Train loss: 0.7925925850868225, Val loss: 0.8321660757064819\n",
      "Epoch: 64, Train loss: 0.790169894695282, Val loss: 0.8294007778167725\n",
      "Epoch: 66, Train loss: 0.790502667427063, Val loss: 0.8415267467498779\n",
      "Epoch: 68, Train loss: 0.7907707691192627, Val loss: 0.8328575491905212\n",
      "Epoch: 70, Train loss: 0.7921064496040344, Val loss: 0.8371853828430176\n",
      "Epoch: 72, Train loss: 0.7899253964424133, Val loss: 0.834513783454895\n",
      "Epoch: 74, Train loss: 0.7910170555114746, Val loss: 0.8333109617233276\n",
      "Epoch: 76, Train loss: 0.790932834148407, Val loss: 0.8334155082702637\n",
      "Epoch: 78, Train loss: 0.7914091944694519, Val loss: 0.8298894166946411\n",
      "Epoch: 80, Train loss: 0.7900078892707825, Val loss: 0.8333539962768555\n",
      "Epoch: 82, Train loss: 0.791675865650177, Val loss: 0.8395392298698425\n",
      "Epoch: 84, Train loss: 0.7913094758987427, Val loss: 0.8345831632614136\n",
      "Epoch: 86, Train loss: 0.79145747423172, Val loss: 0.8349047303199768\n",
      "Epoch: 88, Train loss: 0.7905233502388, Val loss: 0.8341620564460754\n",
      "Epoch: 90, Train loss: 0.7900190353393555, Val loss: 0.8346731662750244\n",
      "Epoch: 92, Train loss: 0.7892486453056335, Val loss: 0.8349027633666992\n",
      "Epoch: 94, Train loss: 0.7894757986068726, Val loss: 0.8333209753036499\n",
      "Epoch: 96, Train loss: 0.7907694578170776, Val loss: 0.8330496549606323\n",
      "Epoch: 98, Train loss: 0.789190411567688, Val loss: 0.833871603012085\n",
      "Epoch: 100, Train loss: 0.7888572216033936, Val loss: 0.8455488085746765\n"
     ]
    }
   ],
   "source": [
    "model = GINENet(num_features, dim_h, edge_attr).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"GIN_0_model_SE.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "df468113-febe-4c3b-8fa1-b5762dfa4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for PNA: 1.055074691772461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234347/3159440653.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"PNA_0_model_SE.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "model.load_state_dict(torch.load(\"PNA_0_model_SE.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068033e-3e9a-4d34-b1cc-20d67ae26e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e2b47804-0c39-4463-9f2a-36c1b0c9dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "\n",
    "def add_extra_node_on_each_edge(data):\n",
    "    # Convert PyG data to a NetworkX graph for easier manipulation\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs = ['edge_attr'])\n",
    "    \n",
    "    # Original number of nodes\n",
    "    num_original_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Prepare lists for new features\n",
    "    edges = list(G.edges(data=True))\n",
    "    new_node_features = []\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_edge_features = []\n",
    "\n",
    "    for u, v, edge_data in edges:\n",
    "        # Remove the original edge\n",
    "        G.remove_edge(u, v)\n",
    "\n",
    "        # Create new node as the mean of connected node features\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        new_node_feature = (data.x[u] + data.x[v]) / 2\n",
    "        new_node_features.append(new_node_feature)\n",
    "        \n",
    "        # Add new node with feature\n",
    "        G.add_node(new_node_id, x=new_node_feature)\n",
    "\n",
    "        # Add edges from new node to each original node\n",
    "        G.add_edge(u, new_node_id)\n",
    "        G.add_edge(new_node_id, v)\n",
    "\n",
    "        # Use original edge feature for each new edge\n",
    "        edge_feature = edge_data['edge_attr']\n",
    "        edge_feature_tensor = (\n",
    "            edge_feature if isinstance(edge_feature, torch.Tensor) else torch.tensor(edge_feature)\n",
    "        )\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (u, new_node_id)\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (new_node_id, v)\n",
    "    \n",
    "    # Convert back to PyG Data object\n",
    "    modified_data = from_networkx(G)\n",
    "\n",
    "    # Update node features\n",
    "    modified_data.x = torch.cat([data.x, torch.stack(new_node_features)], dim=0)\n",
    "\n",
    "    # Update edge features to include only the new edges\n",
    "    modified_data.edge_attr = torch.stack(new_edge_features)  # Only include new edge features\n",
    "\n",
    "    # Preserve any additional global attributes\n",
    "    modified_data.y = data.y\n",
    "    modified_data.smiles = data.smiles\n",
    "    modified_data.name = data.name\n",
    "    modified_data.idx = data.idx\n",
    "    modified_data.pos = data.pos\n",
    "    modified_data.z = data.z\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4d23085d-3c0d-4458-ae1b-359eaac75176",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_ExN = []\n",
    "ExN_set = copy.deepcopy(qm9_dataset)\n",
    "for data in ExN_set:  # Process 100 molecules as an example\n",
    "    modified_data = add_extra_node_on_each_edge(data)\n",
    "    qm9_ExN.append(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8e6b10bd-5468-4934-bfe5-a43524ae7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_exN_dataset = CustomQM9Dataset(qm9_ExN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "90018bdd-fd87-4783-b589-f67e35e4a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_exN_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_exN_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_exN_dataset.data.y = (qm9_exN_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_exN_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_exN_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_exN_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_exN_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_exN_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7223789b-0dd4-4cb9-a3cd-9c7394a4fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.803960382938385, Val loss: 0.8310350179672241\n",
      "Epoch: 4, Train loss: 0.7991498112678528, Val loss: 0.8361106514930725\n",
      "Epoch: 6, Train loss: 0.7971280813217163, Val loss: 0.8348326683044434\n",
      "Epoch: 8, Train loss: 0.7993593215942383, Val loss: 0.8351888656616211\n",
      "Epoch: 10, Train loss: 0.7971411347389221, Val loss: 0.835618793964386\n",
      "Epoch: 12, Train loss: 0.7956464290618896, Val loss: 0.833645761013031\n",
      "Epoch: 14, Train loss: 0.7958497405052185, Val loss: 0.8305310010910034\n",
      "Epoch: 16, Train loss: 0.7952824234962463, Val loss: 0.8333723545074463\n",
      "Epoch: 18, Train loss: 0.7941283583641052, Val loss: 0.8328083157539368\n",
      "Epoch: 20, Train loss: 0.7943302989006042, Val loss: 0.8299891352653503\n",
      "Epoch: 22, Train loss: 0.795461118221283, Val loss: 0.829875111579895\n",
      "Epoch: 24, Train loss: 0.7957280278205872, Val loss: 0.8307376503944397\n",
      "Epoch: 26, Train loss: 0.7950201630592346, Val loss: 0.8311999440193176\n",
      "Epoch: 28, Train loss: 0.7940689325332642, Val loss: 0.8298478722572327\n",
      "Epoch: 30, Train loss: 0.793616771697998, Val loss: 0.8314719796180725\n",
      "Epoch: 32, Train loss: 0.7939015030860901, Val loss: 0.8301647305488586\n",
      "Epoch: 34, Train loss: 0.7939139008522034, Val loss: 0.8337862491607666\n",
      "Epoch: 36, Train loss: 0.7927497029304504, Val loss: 0.8317131400108337\n",
      "Epoch: 38, Train loss: 0.7962584495544434, Val loss: 0.8347179293632507\n",
      "Epoch: 40, Train loss: 0.7969563007354736, Val loss: 0.8305974006652832\n",
      "Epoch: 42, Train loss: 0.7960860729217529, Val loss: 0.8322120904922485\n",
      "Epoch: 44, Train loss: 0.7939558625221252, Val loss: 0.8291823863983154\n",
      "Epoch: 46, Train loss: 0.7939494848251343, Val loss: 0.8318818211555481\n",
      "Epoch: 48, Train loss: 0.7948718070983887, Val loss: 0.8294472694396973\n",
      "Epoch: 50, Train loss: 0.7939022183418274, Val loss: 0.8325071930885315\n",
      "Epoch: 52, Train loss: 0.7982507348060608, Val loss: 0.8296416401863098\n",
      "Epoch: 54, Train loss: 0.7954230308532715, Val loss: 0.8311704993247986\n",
      "Epoch: 56, Train loss: 0.7941138744354248, Val loss: 0.8281176090240479\n",
      "Epoch: 58, Train loss: 0.7942504286766052, Val loss: 0.8350992798805237\n",
      "Epoch: 60, Train loss: 0.7948803901672363, Val loss: 0.8330034613609314\n",
      "Epoch: 62, Train loss: 0.7945981025695801, Val loss: 0.8324401378631592\n",
      "Epoch: 64, Train loss: 0.7954998016357422, Val loss: 0.8312352895736694\n",
      "Epoch: 66, Train loss: 0.7950669527053833, Val loss: 0.8303268551826477\n",
      "Epoch: 68, Train loss: 0.7954714298248291, Val loss: 0.8321369886398315\n",
      "Epoch: 70, Train loss: 0.7955523133277893, Val loss: 0.8325321674346924\n",
      "Epoch: 72, Train loss: 0.794071614742279, Val loss: 0.8310554623603821\n",
      "Epoch: 74, Train loss: 0.7946360111236572, Val loss: 0.8299379348754883\n",
      "Epoch: 76, Train loss: 0.7948976755142212, Val loss: 0.8294007182121277\n",
      "Epoch: 78, Train loss: 0.7930430769920349, Val loss: 0.8331358432769775\n",
      "Epoch: 80, Train loss: 0.7955232858657837, Val loss: 0.8338662385940552\n",
      "Epoch: 82, Train loss: 0.7947348356246948, Val loss: 0.8294710516929626\n",
      "Epoch: 84, Train loss: 0.7939328551292419, Val loss: 0.8371230363845825\n",
      "Epoch: 86, Train loss: 0.7948054075241089, Val loss: 0.8336142301559448\n",
      "Epoch: 88, Train loss: 0.7934888601303101, Val loss: 0.8271251320838928\n",
      "Epoch: 90, Train loss: 0.794460654258728, Val loss: 0.8292853832244873\n",
      "Epoch: 92, Train loss: 0.792935311794281, Val loss: 0.8308852910995483\n",
      "Epoch: 94, Train loss: 0.8017057180404663, Val loss: 0.8325363993644714\n",
      "Epoch: 96, Train loss: 0.7984402775764465, Val loss: 0.832502007484436\n",
      "Epoch: 98, Train loss: 0.7947921752929688, Val loss: 0.8315494656562805\n",
      "Epoch: 100, Train loss: 0.7941913604736328, Val loss: 0.8310157060623169\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_exN_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_exN_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_exN.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c23a3c-7f50-4382-8803-a934d0cb250d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ea1c0be6-559e-40e4-a16f-94d01b159601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_encoding(data):\n",
    "    G = pyg_to_networkx_with_attributes(data)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Initialize the distance matrix with infinity\n",
    "    distance_matrix = [[float('inf')] * num_nodes for _ in range(num_nodes)]\n",
    "    shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    # Populate the distance matrix with actual shortest path lengths\n",
    "    for i in range(num_nodes):\n",
    "        distance_matrix[i][i] = 0  # Distance to self is 0\n",
    "        if i in shortest_paths:\n",
    "            for j, d in shortest_paths[i].items():\n",
    "                distance_matrix[i][j] = d\n",
    "\n",
    "    # Convert the distance matrix to a tensor\n",
    "    distance_tensor = torch.tensor(distance_matrix, dtype=torch.float)\n",
    "    \n",
    "    # Example: Add average distance to node features\n",
    "    finite_distances = torch.where(distance_tensor == float('inf'), torch.tensor(float('nan')), distance_tensor)\n",
    "    average_distance = torch.nanmean(finite_distances, dim=1).view(-1, 1)  # Use nanmean to ignore infinities\n",
    "    data.x = torch.cat([data.x, average_distance], dim=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4e5c2fda-a842-4ebc-b983-67f452bf09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_DE = []\n",
    "DE_set = copy.deepcopy(qm9_dataset)\n",
    "for data in DE_set:  # Process 100 molecules as an example\n",
    "    modified_data = add_distance_encoding(data)\n",
    "    qm9_DE.append(modified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fd731c55-3600-4853-a3e1-1cbb88668577",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_DE_dataset = CustomQM9Dataset(qm9_DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d5b0b509-59c5-459d-8cb5-8126dab17018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_DE_dataset.data.y[0:train_index].mean()\n",
    "data_std = qm9_DE_dataset.data.y[0:train_index].std()\n",
    "\n",
    "qm9_DE_dataset.data.y = (qm9_DE_dataset.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_DE_dataset[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_DE_dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_DE_dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_DE_dataset[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_DE_dataset[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "cf439ae0-1edd-47d4-90c1-5f1ee60b6ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.805455207824707, Val loss: 0.8395050168037415\n",
      "Epoch: 4, Train loss: 0.7989227175712585, Val loss: 0.835909366607666\n",
      "Epoch: 6, Train loss: 0.7978875041007996, Val loss: 0.8300177454948425\n",
      "Epoch: 8, Train loss: 0.7981917262077332, Val loss: 0.8379428386688232\n",
      "Epoch: 10, Train loss: 0.7997327446937561, Val loss: 0.8329315185546875\n",
      "Epoch: 12, Train loss: 0.7966440916061401, Val loss: 0.8342440128326416\n",
      "Epoch: 14, Train loss: 0.7961634993553162, Val loss: 0.8330127596855164\n",
      "Epoch: 16, Train loss: 0.7968370914459229, Val loss: 0.8309316635131836\n",
      "Epoch: 18, Train loss: 0.7977918386459351, Val loss: 0.8349006175994873\n",
      "Epoch: 20, Train loss: 0.7949866652488708, Val loss: 0.8292779922485352\n",
      "Epoch: 22, Train loss: 0.7959179878234863, Val loss: 0.8339409828186035\n",
      "Epoch: 24, Train loss: 0.7962321639060974, Val loss: 0.8338037729263306\n",
      "Epoch: 26, Train loss: 0.7971914410591125, Val loss: 0.8290377855300903\n",
      "Epoch: 28, Train loss: 0.7944954633712769, Val loss: 0.8289449214935303\n",
      "Epoch: 30, Train loss: 0.7970820069313049, Val loss: 0.83005690574646\n",
      "Epoch: 32, Train loss: 0.7985512018203735, Val loss: 0.8338020443916321\n",
      "Epoch: 34, Train loss: 0.7969434857368469, Val loss: 0.8320028185844421\n",
      "Epoch: 36, Train loss: 0.7935240864753723, Val loss: 0.8299592733383179\n",
      "Epoch: 38, Train loss: 0.7958942651748657, Val loss: 0.8310834169387817\n",
      "Epoch: 40, Train loss: 0.7953502535820007, Val loss: 0.8351776599884033\n",
      "Epoch: 42, Train loss: 0.7940046787261963, Val loss: 0.8303860425949097\n",
      "Epoch: 44, Train loss: 0.795479953289032, Val loss: 0.8320962190628052\n",
      "Epoch: 46, Train loss: 0.7964222431182861, Val loss: 0.834277868270874\n",
      "Epoch: 48, Train loss: 0.7949857711791992, Val loss: 0.8271982073783875\n",
      "Epoch: 50, Train loss: 0.794590413570404, Val loss: 0.830978512763977\n",
      "Epoch: 52, Train loss: 0.7957683801651001, Val loss: 0.8335299491882324\n",
      "Epoch: 54, Train loss: 0.7943394184112549, Val loss: 0.8312621116638184\n",
      "Epoch: 56, Train loss: 0.7942361831665039, Val loss: 0.8305373787879944\n",
      "Epoch: 58, Train loss: 0.7953831553459167, Val loss: 0.8308877348899841\n",
      "Epoch: 60, Train loss: 0.7934241890907288, Val loss: 0.8322953581809998\n",
      "Epoch: 62, Train loss: 0.7950243949890137, Val loss: 0.8274429440498352\n",
      "Epoch: 64, Train loss: 0.7944069504737854, Val loss: 0.8292961120605469\n",
      "Epoch: 66, Train loss: 0.7949321269989014, Val loss: 0.8290042281150818\n",
      "Epoch: 68, Train loss: 0.7945346236228943, Val loss: 0.8308722972869873\n",
      "Epoch: 70, Train loss: 0.7942429184913635, Val loss: 0.8373329043388367\n",
      "Epoch: 72, Train loss: 0.794352650642395, Val loss: 0.8378267288208008\n",
      "Epoch: 74, Train loss: 0.7943146824836731, Val loss: 0.8308894038200378\n",
      "Epoch: 76, Train loss: 0.7962040901184082, Val loss: 0.8328466415405273\n",
      "Epoch: 78, Train loss: 0.7943532466888428, Val loss: 0.8286086320877075\n",
      "Epoch: 80, Train loss: 0.7946966886520386, Val loss: 0.8292382955551147\n",
      "Epoch: 82, Train loss: 0.7965832948684692, Val loss: 0.8293715715408325\n",
      "Epoch: 84, Train loss: 0.7939942479133606, Val loss: 0.8290414810180664\n",
      "Epoch: 86, Train loss: 0.7940389513969421, Val loss: 0.8305031657218933\n",
      "Epoch: 88, Train loss: 0.7952831983566284, Val loss: 0.8305240869522095\n",
      "Epoch: 90, Train loss: 0.7964001893997192, Val loss: 0.8309592008590698\n",
      "Epoch: 92, Train loss: 0.7947486639022827, Val loss: 0.828719973564148\n",
      "Epoch: 94, Train loss: 0.7951468229293823, Val loss: 0.8305978178977966\n",
      "Epoch: 96, Train loss: 0.7931773066520691, Val loss: 0.8307576179504395\n",
      "Epoch: 98, Train loss: 0.7945674657821655, Val loss: 0.8361551761627197\n",
      "Epoch: 100, Train loss: 0.7968904972076416, Val loss: 0.8303167223930359\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_DE_dataset[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_DE_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_DE.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "64a8ac11-dd0f-4be4-b0b3-484db97773e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhifei/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "qm9_GE = copy.deepcopy(qm9_dataset)\n",
    "\n",
    "transform = AddLaplacianEigenvectorPE(k=2, attr_name = None)\n",
    "qm9_GE.transform = transform\n",
    "\n",
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_GE.data.y[0:train_index].mean()\n",
    "data_std = qm9_GE.data.y[0:train_index].std()\n",
    "\n",
    "qm9_GE.data.y = (qm9_GE.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_GE[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_GE[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_GE[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_GE[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_GE[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4eabc347-0c3e-4746-b2be-28031e6ae7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.7951242327690125, Val loss: 0.8293468356132507\n",
      "Epoch: 4, Train loss: 0.7965684533119202, Val loss: 0.8285439014434814\n",
      "Epoch: 6, Train loss: 0.7910226583480835, Val loss: 0.8341351747512817\n",
      "Epoch: 8, Train loss: 0.7912823557853699, Val loss: 0.8266131281852722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[337], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pna_best_loss, pna_train_loss, pna_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNA_0_model_GE.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[216], line 29\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     28\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m train_loss[epoch] \u001b[38;5;241m=\u001b[39m epoch_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m val_loss[epoch] \u001b[38;5;241m=\u001b[39m v_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[211], line 45\u001b[0m, in \u001b[0;36mPNANet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pass node features and edge attributes through PNA layers\u001b[39;00m\n\u001b[1;32m     44\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index, edge_attr)\n\u001b[0;32m---> 45\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(h2, edge_index, edge_attr)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Apply global pooling for graph-level output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/pna_conv.py:167\u001b[0m, in \u001b[0;36mPNAConv.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    164\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mF_in)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtowers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    170\u001b[0m outs \u001b[38;5;241m=\u001b[39m [nn(out[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, nn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_nns)]\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.pna_conv_PNAConv_propagate_2ie_ivjl.py:186\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    176\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    177\u001b[0m                 x_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/pna_conv.py:180\u001b[0m, in \u001b[0;36mPNAConv.message\u001b[0;34m(self, x_i, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    178\u001b[0m h: Tensor \u001b[38;5;241m=\u001b[39m x_i  \u001b[38;5;66;03m# Dummy.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m edge_attr\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mF_in)\n\u001b[1;32m    182\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m edge_attr\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtowers, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_GE[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_GE[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_GE.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e4090704-8471-4abc-8f60-88f0ee4a1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU, Linear, ReLU, Sequential\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "\n",
    "target = 0\n",
    "dim = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MyTransform:\n",
    "    def __call__(self, data):\n",
    "        data = copy.copy(data)\n",
    "        data.y = data.y[:, target]  # Specify target.\n",
    "        return data\n",
    "\n",
    "\n",
    "class Complete:\n",
    "    def __call__(self, data):\n",
    "        data = copy.copy(data)\n",
    "        device = data.edge_index.device\n",
    "\n",
    "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
    "        col = col.repeat(data.num_nodes)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        edge_attr = None\n",
    "        if data.edge_attr is not None:\n",
    "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
    "            size = list(data.edge_attr.size())\n",
    "            size[0] = data.num_nodes * data.num_nodes\n",
    "            edge_attr = data.edge_attr.new_zeros(size)\n",
    "            edge_attr[idx] = data.edge_attr\n",
    "\n",
    "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "        data.edge_attr = edge_attr\n",
    "        data.edge_index = edge_index\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
    "dataset = QM9(root='dataset/QM9', transform=transform).shuffle()\n",
    "qm9_dataset = dataset[:5000]\n",
    "\n",
    "# normalizing the data\n",
    "mean = qm9_dataset.data.y[1000:5000].mean()\n",
    "std = qm9_dataset.data.y[1000:5000].std()\n",
    "qm9_dataset.data.y = (qm9_dataset.data.y - mean) / std\n",
    "\n",
    "# Split datasets.\n",
    "test_dataset = dataset[:500]\n",
    "val_dataset = dataset[500:1000]\n",
    "train_dataset = dataset[1000:5000]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5b2b3850-d29c-4ccf-91a2-ea3e28769e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, LR: 0.001000, Loss: 0.0081864, Val MAE: 12.6128933, Test MAE: 13.2576230\n",
      "Epoch: 002, LR: 0.001000, Loss: 0.0000360, Val MAE: 9.9032644, Test MAE: 9.6857236\n",
      "Epoch: 003, LR: 0.001000, Loss: 0.0000047, Val MAE: 4.5216296, Test MAE: 4.3478125\n",
      "Epoch: 004, LR: 0.001000, Loss: 0.0000025, Val MAE: 4.2012156, Test MAE: 4.1506926\n",
      "Epoch: 005, LR: 0.001000, Loss: 0.0000019, Val MAE: 3.6806682, Test MAE: 3.6976677\n",
      "Epoch: 006, LR: 0.001000, Loss: 0.0000017, Val MAE: 3.6606716, Test MAE: 3.6427783\n",
      "Epoch: 007, LR: 0.001000, Loss: 0.0000014, Val MAE: 3.2824202, Test MAE: 3.3308196\n",
      "Epoch: 008, LR: 0.001000, Loss: 0.0000013, Val MAE: 3.0237275, Test MAE: 3.0077478\n",
      "Epoch: 009, LR: 0.001000, Loss: 0.0000011, Val MAE: 2.8914763, Test MAE: 2.9436387\n",
      "Epoch: 010, LR: 0.001000, Loss: 0.0000010, Val MAE: 2.6621523, Test MAE: 2.7135486\n",
      "Epoch: 011, LR: 0.001000, Loss: 0.0000009, Val MAE: 2.7388948, Test MAE: 2.7135486\n",
      "Epoch: 012, LR: 0.001000, Loss: 0.0000008, Val MAE: 2.4162644, Test MAE: 2.4829402\n",
      "Epoch: 013, LR: 0.001000, Loss: 0.0000007, Val MAE: 2.1351865, Test MAE: 2.1855674\n",
      "Epoch: 014, LR: 0.001000, Loss: 0.0000006, Val MAE: 2.1306406, Test MAE: 2.2275950\n",
      "Epoch: 015, LR: 0.001000, Loss: 0.0000006, Val MAE: 2.1016113, Test MAE: 2.3101345\n",
      "Epoch: 016, LR: 0.001000, Loss: 0.0000005, Val MAE: 1.7750386, Test MAE: 1.9346868\n",
      "Epoch: 017, LR: 0.001000, Loss: 0.0000004, Val MAE: 1.5469839, Test MAE: 1.6184805\n",
      "Epoch: 018, LR: 0.001000, Loss: 0.0000004, Val MAE: 1.4319050, Test MAE: 1.5105728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[342], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m     65\u001b[0m     lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     val_error \u001b[38;5;241m=\u001b[39m test(val_loader)\n\u001b[1;32m     68\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_error)\n",
      "Cell \u001b[0;32mIn[342], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(model(data), data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     47\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 48\u001b[0m     loss_all \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_graphs\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_all \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import NNConv, Set2Set\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
    "\n",
    "        nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
    "        self.gru = GRU(dim, dim)\n",
    "\n",
    "        self.set2set = Set2Set(dim, processing_steps=3)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = F.relu(self.lin0(data.x))\n",
    "        h = out.unsqueeze(0)\n",
    "\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=0.7, patience=5,\n",
    "                                                       min_lr=0.00001)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(model(data), data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
    "    return error / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_error = None\n",
    "for epoch in range(1, 101):\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    val_error = test(val_loader)\n",
    "    scheduler.step(val_error)\n",
    "\n",
    "    if best_val_error is None or val_error <= best_val_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_val_error = val_error\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
    "          f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d69fb99e-babe-4f32-ad02-1e7120187371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU, Linear, ReLU, Sequential\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "\n",
    "target = 0\n",
    "dim = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MyTransform:\n",
    "    def __call__(self, data):\n",
    "        data = copy.copy(data)\n",
    "        data.y = data.y[:, target]  # Specify target.\n",
    "        return data\n",
    "\n",
    "\n",
    "class Complete:\n",
    "    def __call__(self, data):\n",
    "        data = copy.copy(data)\n",
    "        device = data.edge_index.device\n",
    "\n",
    "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
    "        col = col.repeat(data.num_nodes)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        edge_attr = None\n",
    "        if data.edge_attr is not None:\n",
    "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
    "            size = list(data.edge_attr.size())\n",
    "            size[0] = data.num_nodes * data.num_nodes\n",
    "            edge_attr = data.edge_attr.new_zeros(size)\n",
    "            edge_attr[idx] = data.edge_attr\n",
    "\n",
    "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "        data.edge_attr = edge_attr\n",
    "        data.edge_index = edge_index\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
    "dataset = QM9(root='dataset/QM9', transform=transform).shuffle()\n",
    "qm9_dataset = dataset[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1857f6de-ad74-4ea9-a293-270d9e8d2821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[16, 11], edge_index=[2, 240], edge_attr=[240, 5], y=[1], pos=[16, 3], z=[16], smiles='[H]N([H])[C@H]1[NH2+]C([H])([H])[C@]([H])(OC([H])([H])[H])O1', name='gdb_11407', idx=[1])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0258066b-f0c2-4aa6-b6ea-9efbce6d1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "qm9_GE = copy.deepcopy(qm9_dataset)\n",
    "\n",
    "transform = AddLaplacianEigenvectorPE(k=2, attr_name = None)\n",
    "qm9_GE.transform = transform\n",
    "\n",
    "# data split\n",
    "data_size = 5000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9_GE.data.y[0:train_index].mean()\n",
    "data_std = qm9_GE.data.y[0:train_index].std()\n",
    "\n",
    "qm9_GE.data.y = (qm9_GE.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9_GE[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9_GE[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9_GE[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "# Determine the maximum possible degree, ensuring it's an integer\n",
    "max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in qm9_GE[:train_index]]))\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "\n",
    "# Compute the in-degree histogram over the training data\n",
    "for data in qm9_GE[:train_index]:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "461b388d-0b19-4f08-8f8b-9a4b04dbfd08",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 1]' is invalid for input of size 1216",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[345], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Remember to change the path if you want to keep the previously trained model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pna_best_loss, pna_train_loss, pna_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPNA_0_model_GE.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[216], line 22\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs):\n\u001b[0;32m---> 22\u001b[0m     epoch_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     v_loss \u001b[38;5;241m=\u001b[39m validation(val_loader, model, loss)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[0;32mIn[212], line 23\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(loader, model, loss, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 23\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(out, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m     25\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 1]' is invalid for input of size 1216"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_features = qm9_GE[0].x.shape[1]\n",
    "dim_h = 64\n",
    "edge_attr = qm9_GE[0].edge_attr.shape[1]\n",
    "\n",
    "model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(\n",
    "    epochs, model, train_loader, val_loader, \"PNA_0_model_GE.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f162b-13f5-4779-a370-edfbc0b081b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
